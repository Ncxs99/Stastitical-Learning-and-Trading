{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc10879",
   "metadata": {},
   "source": [
    "## PROJET DE DEEP LEARNING : AUDIO BIRD CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d112583",
   "metadata": {},
   "source": [
    "## NOTEBOOK DU PROJET ET RESULTATS OBTENUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4b6a1ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer les librairies importantes \n",
    "import math, random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from torch import nn\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503d3154",
   "metadata": {},
   "source": [
    "> En deep learning, pour accélerer les calculs, nous devons posséder un GPU ( Graphical Processing Unit ). Un GPU est une composante électronique de notre pc qui permet d'accélerer le processus d'entrainement des données. Si le GPU n'est pas disponible, par défaut notre libraire de DL utilise le CPU ( Central Processing Unit ).\n",
    "\n",
    "\n",
    "\n",
    "> Vérifions si Pytorch utilise bien un accélérateur qui sera le GPU de notre pc par défaut dénommé : mps. Si le GPU est bien disponible pour accélerer les calculs, notre commande va nous retourner True sinon ce sera False dans le cas contraire et ce sera le CPU qui sera utilisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fa9bd274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa90992",
   "metadata": {},
   "source": [
    "    Un accélerateur de calculs est donc présent ce qui signifie que Pytorch utilise bien le GPU de notre PC "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f21705",
   "metadata": {},
   "source": [
    "> Pour pouvoir utiliser le GPU, nous devons configurer Pytorch de sorte a ce qu'il fasse tourner les tenseurs sur le GPU. Pour ce faire nous devons définir une variable nommée **device** qui va contenir le nom de notre GPU. De cette facon, nous allons faire tourner tout sur GPU pour accélerer les calculs au travers de notre variable **device**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "41519e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c1ec34ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae7f25",
   "metadata": {},
   "source": [
    "    Pytorch est maintenant configuré pour tourner les calculs sur un certain accélérateur qui s'appelle device qui se trouve etre mps qui est notre GPU sur pc. Pour faire tourner les calculs sur GPU, nous devons explicitement déclaré cela a Pytorch dans le code auquel cas nous serons toujours sur CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0dea5d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par exemple, définissons simplement un tenseur : \n",
    "a = torch.tensor([1,3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5fcbc345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 2])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a    # notre tenseur a ne tourne pas sur GPU mais sur CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "478d6381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essayons maintenant ce code \n",
    "b = torch.tensor([1,3,2]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fb6c901a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 2], device='mps:0')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b   # notre tenseur tourne bien sur GPU et non pas sur CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9e054844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [87]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Essayons de faire une opération pour voir ce que cela donne : \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!"
     ]
    }
   ],
   "source": [
    "# Essayons de faire une opération pour voir ce que cela donne : \n",
    "\n",
    "a + b # Comme nous le voyons, les deux tenseurs opèrent sur deux différents 'devices', l'un sur CPU et l'autre GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a792da",
   "metadata": {},
   "source": [
    "    Dans toute la suite de notre projet, nous ferons tourner tous nos calculs sur GPU pour accélerer les calculs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c34e267",
   "metadata": {},
   "source": [
    "## PREPROCESSING DES DONNÉES "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda3f706",
   "metadata": {},
   "source": [
    "    Nous allons définir maintenant via les librairie os et pathlib le répertoire de travail pour pouvoir importer nos données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a06ce96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/nacersere/') # nous définissons notre répertoire de travail pour faciliter l'importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "92463ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = Path.cwd()/'Downloads'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e3977fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file = download_path/'ff1010bird_metadata.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db959110",
   "metadata": {},
   "source": [
    "> Nous importons tout d'abord la métadonnée qui contient tous les labels, donc les variables a prédire des audios du fichier central téléchargé sur le site. C'est donc une information éssentielle pour notre projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "40f3a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(metadata_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "88c2fc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>hasbird</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64486</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44981</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101323</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>165746</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   itemid  hasbird\n",
       "0   64486        0\n",
       "1    2525        0\n",
       "2   44981        0\n",
       "3  101323        0\n",
       "4  165746        0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()   # 5 premières lignes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "66961ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>hasbird</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7685</th>\n",
       "      <td>168059</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7686</th>\n",
       "      <td>164922</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7687</th>\n",
       "      <td>80789</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7688</th>\n",
       "      <td>104733</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7689</th>\n",
       "      <td>40565</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      itemid  hasbird\n",
       "7685  168059        0\n",
       "7686  164922        0\n",
       "7687   80789        1\n",
       "7688  104733        1\n",
       "7689   40565        0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()  # 5 dernières lignes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a431003b",
   "metadata": {},
   "source": [
    "> A partir de cette étape, nous allons adopter une stratégie qui nous permet d'importer nos données de manière efficiente. Nous savons que la base de donnée qui contient nos audios téléchargés est assez volumineuse : 6.5 GB. Importer toute la base de donnée directement sur Pandas est inefficient : Long temps de chargement et de calculs & consommation de mémoire inutile. \n",
    "\n",
    "> Par contre, il existe une manière bien plus efficiente d'importer la base donnée : une approche par **lien de répertoire**. Elle consiste a définir dans notre métadonnée importée, une nouvelle colonne contenant pour chaque observation de la métadonnée et donc de chaque label (0 ou 1) par la meme occasion, un lien de répertoire qui servirait a **référencer** l'audio correspondant dans le gros fichier téléchargé. Ainsi, plutot que d'importer toute la base audio, lors du processing, Python va chercher chaque audio dans le gros fichier au travers du **lien de répertoire** soigneusement défini au préalable, et le processer. On importe donc de manière **séquentielle** et non pas de manière linéaire nos audios. \n",
    "\n",
    "\n",
    "> Remarquons cependant que l'identifiant des audios dans la métadonnée (colonne itemid) correspond a l'identifiant des audios du gros fichier téléchargé sans l'extension .wav .Pour aider Python a réferencer chaque audio via un lien, nous procédons comme suit : \n",
    "\n",
    "   - Convertissons la colonne itemid en chaine de caractères\n",
    "   - Nous ajoutons a cette colonne un '/' pour créer un chemin de répertoire \n",
    "   - Nous lui ajoutons enfin une extension .wav a la fin ( crucial ! ) car cela permet de référencer directement les audios depuis la base de donnée volumineuse. De cette manière on se retrouve avec une métadonnée qui contient pour chaque audio de la base de donnée au travers de son lien correspondant, le label qui lui sied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f8f18529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "itemid     int64\n",
       "hasbird    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes    # Commande qui retourne le type des variables de notre dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d1e775dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['itemid'] = df['itemid'].astype('str') # convertion de la colonne en chaine de caractère "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4f1276d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "itemid     object\n",
       "hasbird     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cad54a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['relative_path'] = '/' + df['itemid'] + '.wav' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583c4090",
   "metadata": {},
   "source": [
    "    relative_path contient maintenant, pour chaque label (0 ou 1), le lien du fichier audio correspondant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "93d90083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>hasbird</th>\n",
       "      <th>relative_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64486</td>\n",
       "      <td>0</td>\n",
       "      <td>/64486.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2525</td>\n",
       "      <td>0</td>\n",
       "      <td>/2525.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44981</td>\n",
       "      <td>0</td>\n",
       "      <td>/44981.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101323</td>\n",
       "      <td>0</td>\n",
       "      <td>/101323.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>165746</td>\n",
       "      <td>0</td>\n",
       "      <td>/165746.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   itemid  hasbird relative_path\n",
       "0   64486        0    /64486.wav\n",
       "1    2525        0     /2525.wav\n",
       "2   44981        0    /44981.wav\n",
       "3  101323        0   /101323.wav\n",
       "4  165746        0   /165746.wav"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8d3a37a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['relative_path', 'hasbird']]  # nous gardons seulement le label & relative_path dans le dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b879352a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_path</th>\n",
       "      <th>hasbird</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/64486.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/2525.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/44981.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/101323.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/165746.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  relative_path  hasbird\n",
       "0    /64486.wav        0\n",
       "1     /2525.wav        0\n",
       "2    /44981.wav        0\n",
       "3   /101323.wav        0\n",
       "4   /165746.wav        0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "692f33cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_path</th>\n",
       "      <th>hasbird</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7685</th>\n",
       "      <td>/168059.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7686</th>\n",
       "      <td>/164922.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7687</th>\n",
       "      <td>/80789.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7688</th>\n",
       "      <td>/104733.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7689</th>\n",
       "      <td>/40565.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     relative_path  hasbird\n",
       "7685   /168059.wav        0\n",
       "7686   /164922.wav        0\n",
       "7687    /80789.wav        1\n",
       "7688   /104733.wav        1\n",
       "7689    /40565.wav        0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2a3c2e",
   "metadata": {},
   "source": [
    "> Une fois trouvée une manière d'importer les données, passons maintenant au préprocessing. Nous devons préprocesser nos audios avant de les introduire dans un réseau de neurones. Voici comment nous avons procéder étape par étape : \n",
    "\n",
    "- Nous avons créer une classe AudioUtil qui contient toutes nos étapes de Préprocessing des audios. Nous avons jugé bon d'utliser de l'orienté objet car cela est en accord avec Pytorch et cela nous permet d'assurer une certaine scalabilité et une certaine reproductibilité du code. Ainsi nous allons juste instancier, passer les paramètres, ce qui réduit consdérablement le temps de développement.\n",
    "\n",
    "                Cette classe contient les fonctions de preprocessing suivantes par ordre : \n",
    " \n",
    "- La fonction **open()** : La première étape de preprocessing est la **numérisation des fichiers audios** ou le fait de transformer les audios du fichier volumineux en nombres puis au travers d'un modèle de DL, trouver des relations dans ces nombres. Pour cela nous utilisons une fonction **open()**, qui prend en paramètre le lien du répertoire de la métadonée et convertit au travers d'une fonction de Pytorch, **torchaudio.load()**, l'audio correspondant dans le fichier volumineux en **torch.tensor()**, l'équivalent des **numpy.array()** en quelque sortes. Autrement dit, **torchaudio.load()** convertit les audios en tableaux de nombres. la fonction **torch.load()** retourne deux variables importantes toutes contenues dans un tuple : \n",
    "\n",
    "> Le premier élément du tuple est l'équivalent numérique des fichiers audios. Autrement dit, ce sont les audios mais représenté sous forme de tableaux de nombres. \n",
    "   \n",
    "> Le second élément tout aussi important est le **sample rate**. Vu que les audios par définition sont des ondes qui se déplacent, et qu'une onde a une **amplitude** et une **fréquence** donnée, le **sample rate** est donc le nombre d'amplitudes que l'on a du calculer par seconde pour convertir notre audio en tableaux de nombres. Autrement dit, un audio de 00:10 secondes aura une dimension de 441.000 puisque par seconde d'enregistrement, ce sont 44.100 amplitudes qui sont calculées (44.100) est la valeur par défaut couramment utilisé en théorie du signal et machine learning. C'est la valeur par défaut utilisée par la libraire torchaudio de Pytorch.\n",
    "\n",
    "\n",
    "- La fonction **rechannel()** : Cette fonction permet de convertir des audios monos en audios stéréos et inversement. En effet, certains audios sont enregistrés en mono et d'autres enregistrés en stéréo. Vu que notre modèle de DL s'attend a ce que tous les tenseurs aient la meme dimension, il est important de convertir tous les audios sur la meme chaine : mono ou stéréo. Dans la suite de notre projet, nous avons décidé de convertir tous les audios en **stéréo**. \n",
    "\n",
    "\n",
    "- La fonction **resample()** : Cette fonction permet d'uniformiser le sample rate des audios pour que leurs représentations numériques aient la meme dimension. En effet, certains audios sont enregistrés avec un sample rate de par exemple 50.000. Cela signifie que le tableau de nombre équivalent a un audio de 00:10 secondes enregistré avec un sample rate de 50.000 sera de dimension 500.000 alors que celui d'un meme audio de 00:10 secondes enregistré avec un sample rate standard de 44.100 sera de dimension 441.000. Comment alors uniformiser les dimensions pour notre résau de neurones et éviter une erreur de dimenion ?. C'est la ou vient notre fonction **resample()**.  \n",
    "\n",
    "\n",
    "- La fonction **pad_trunc()** : Cette fonction nous permet d'uniformiser cette fois **la durée des audios**. En effet, un tableau numérique d'un audio de 00:10 secondes enregistré avec un **sample rate** de 44.100 secondes aura une dimension de 441.000 alors qu'un audio de 00:20 secondes enregistrés avec un **sample rate** de 44.100 secondes aura une dimension de 20 * 44.100, soit deux fois plus que le premier. La fonction **pad_trunc** nous permet donc soit d'augmenter la durée des audios en ajoutant du silence soit en le diminuant. \n",
    "\n",
    "> En réalité, vu que nos audios sont tous de 00:10 secondes dans le fichier volumineux, cette étape de préprocessing n'était pas nécessaire puisque par défaut tous les audios sur lesquels nous travaillons aurons la meme dimension. Seul le **sample rate** peut varier, mais vu que en machine learning nous devons garder la généralisation d'un modèle en tete lors de sa conception et que nous ne savons pas comment et dans quelles condition seront les données de test, il est important de tout incorporer soigneusement. \n",
    "    \n",
    "    \n",
    "- La fonction **time_shift()** : Une technique courante pour **augmenter la diversité** d'un ensemble de données, en particulier lorsque l'on ne dispose pas de suffisamment de données, consiste à **augmenter artificiellement** nos données. Pour ce faire, nous **modifions légèrement** les échantillons de données existants. Par exemple, avec les images, nous pouvons faire des choses comme faire légèrement pivoter l'image, la recadrer ou la redimensionner, modifier les couleurs ou l'éclairage, ou ajouter du bruit à l'image. Étant donné que la sémantique de l'image n'a pas changé de manière significative, la même étiquette cible de l'échantillon d'origine s'appliquera toujours à l'échantillon augmenté. Par exemple, si l'image était étiquetée comme un \"chat\", l'image augmentée sera également un \"chat\". **Mais, du point de vue du modèle, cela ressemble à un nouvel échantillon de données.** Cela aide notre modèle à généraliser à une plus grande gamme d'entrées d'image. Tout comme pour les images, **il existe plusieurs techniques pour augmenter également les données audio**. Cette augmentation peut être faite à la fois sur l'audio brut avant de produire le spectrogramme, ou sur le spectrogramme généré. L'augmentation du spectrogramme produit généralement de meilleurs résultats.\n",
    "\n",
    "\n",
    "> Le **time_shift()** est par conséquent une astuce classique de Preprocessing en classification audio consistant a augmenter les données sur le signal audio brut en lui appliquant un décalage temporel pour décaler l'audio vers la gauche ou la droite d'une quantité aléatoire. Cette augmentation crée artificiellement une diversité dans nos données et permet a notre modèle de généraliser plus facilement. Cet décalage permet d'avoir de très bons résultats comme on le verra par la suite et est une astuce parmi plusieurs autres. La fonction **time_shift** permet par conséquent d'augmenter artificellement nos données\n",
    "\n",
    "\n",
    "- La fonction **specto_gram()** : Cette fonction nous permet de convertir ensuite l'audio augmenté en un spectrogramme. Ils capturent les caractéristiques essentielles de l'audio et constituent souvent le moyen le plus approprié d'entrer des données audio dans des modèles de DL. Le spectogram est par définition basique,la représentation la plus fidèle sous forme d'image de ce que serait les audios dans le vide. C'est un graphe qui pour chaque durée de l'audio, on lui fait correspondre la fréquence correspondante. Le spectogram étant sous forme d'image, **il convient parfaitement a un réseau de neurones convolutionnels**. La fonction **spectro_gram** permet de convertir nos audios augmentés en image. \n",
    "\n",
    "\n",
    "- La fonction **spectro_augment()** : Maintenant, nous pouvons faire une autre série d'augmentations, cette fois sur le Spectrogram plutôt que sur l'audio brut. L'objectif étant le meme que l'augmentation du fichier audio. Astuce classique en machine learning pour la performance et le pouvoir de généralisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0084f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioUtil():\n",
    "    \n",
    "    @staticmethod\n",
    "    def open(audio_file):\n",
    "        sig, sr = torchaudio.load(audio_file)\n",
    "        return (sig, sr)\n",
    "    \n",
    "    @staticmethod\n",
    "    def rechannel(aud,new_channel):\n",
    "        sig, sr = aud\n",
    "        \n",
    "        if (sig.shape[0] == new_channel):\n",
    "            return aud \n",
    "        \n",
    "        if (new_channel == 1):\n",
    "            resig = sig[:1,:]\n",
    "            \n",
    "        else:\n",
    "            resig = torch.cat([sig,sig])\n",
    "            \n",
    "        return ((resig, sr))\n",
    "    \n",
    "    @staticmethod\n",
    "    def resample(aud, newsr):\n",
    "        sig, sr = aud\n",
    "        \n",
    "        if (sr == newsr):\n",
    "            return aud\n",
    "        \n",
    "        num_channels = sig.shape[0]\n",
    "        \n",
    "        resig = torchaudio.transforms.Resample(sr,newsr)(sig[:1,:])\n",
    "        \n",
    "        if (num_channels > 1):\n",
    "            \n",
    "            retwo = torchaudio.transforms.Resample(sr,newsr)(sig[1:,:])\n",
    "            resig = torch.cat([resig,retwo])\n",
    "        \n",
    "        return ((resig,newsr))\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_trunc(aud,max_ms):\n",
    "        sig, sr = aud\n",
    "        num_rows, sig_len = sig.shape\n",
    "        max_len = sr//1000 * max_ms\n",
    "        \n",
    "        if (sig_len > max_len):\n",
    "            sig = sig[:,:max_len]\n",
    "            \n",
    "        elif (sig_len < max_len):\n",
    "            pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "            pad_end_len = max_len - sig_len - pad_begin_len \n",
    "            \n",
    "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "            pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "            \n",
    "            sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "            \n",
    "        return (sig, sr)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def time_shift(aud,shift_limit):\n",
    "        sig, sr = aud\n",
    "        _,sig_len = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "        \n",
    "        return (sig.roll(shift_amt), sr)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        sig, sr = aud\n",
    "        top_db = 80\n",
    "        \n",
    "        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "        \n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        \n",
    "        return (spec)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        _, n_mels , n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "        \n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "            \n",
    "            time_mask_param = max_mask_pct * n_steps\n",
    "            \n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "            \n",
    "            return aug_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed28f384",
   "metadata": {},
   "source": [
    "> Une fois le preprocessing effectué, nous allons créer une classe **SoundDs** en héritant de la classe **Dataset** de Pytorch pour permettre d'appeler chaque fonction de préprocessing décrite précédemment tout en faisant passer les paramètres qui **caractérisent nos données**. \n",
    "\n",
    "\n",
    "> Notre idée est la suivante. Nous allons créer la classe **SoundDS** pour importer les audios proprement dit au travers des liens de répertoire et les processer au fur et a mésure en appelant les fonctions de notre classe **AudioUtil**. Nous allons par conséquent processer de manière **séquentielle** chaque audio, au travers de notre nouvelle classe. Voici comment nous procédons : \n",
    "\n",
    "- La programmation orientée objet utilisée précedemment dans le preprocessing prend tout son sens. En effet, il nous suffit d'appeler maintenant juste la classe précédente tout en passant dans les méthodes de cette classe les paramètres qui nous conviennent et qui sont propres a nos données. Cela nous génère un gain de temps en développement énorme, et permet ainsi de maintenir le code sur la durée et de ne pas revenir fréquemment regarder les paramètres des fonctions pour faire passer les arguments. On définit par conséquent la classe **SoundDS**\n",
    "\n",
    "\n",
    "- On crée les fonctions habituelles comme dans une programmation orientée objet classique avec la fonction $__init__()$ . On utilise la fonction $__len__()$ qui est une surcharge d'opérateur pour nous permettre d'utiliser la fonction **len()** sur l'objet que l'on va créer au travers de notre classe **SoundDS** sous Python. Cet surcharge d'opérateur sera importante car elle nous permettra de spliter notre base de donnée en base de donnée d'entrainement et de test et meme de faire plus tard de la validation corisée. On utilise également la fonction $__getitem__()$ qui nous permet, de la meme manière que l'on utilise sur les listes, les numpy arrays ou les tenseurs, d'utiliser les $[]$ sur cette fois l'objet que l'on créer au travers de notre classe **SoundDS**\n",
    "\n",
    "\n",
    "- Enfin, on instancie notre classe **AudioUtil** avec les paramètres caractéristiques de nos données:\n",
    "\n",
    "    * **AudioUtil.open('/..../')** prend en paramètre le lien de répertoire des audios\n",
    "   \n",
    "    * **AudioUtil.resample(aud, self.sr)** prend en paramètres : le **résultat** de la fonction AudioUtil.open() qui est représenté par le paramètre **aud**. **aud** est en fait un tuple qui contient rappelons le deux résultats : La réprésentation sous forme de tableau des audios et le **sample rate avec lequel les audios ont été nativement enregistrés et dont Pytorch s'est servi pour encoder automatiquement ces derniers**. Vu que ces **sample rate natifs** peuvent différer, la fontion AudioUtil.resample() uniformise donc la représentation des audios sous forme de nombres issus de la fonction AudioUtil.open() avec le meme **sample rate** et ce **sample rate** est le paramètre **self.sr**. Ce **sample rate** sera le **sample rate** par défaut en pratique : 44.100.  A cette étape donc, tous nos fichiers audios sont encodés numériquement sous forme de tableau et ont la meme dimension.\n",
    "   \n",
    "    * **AudioUtil.rechannel(reaud, self.channel)** processe les audios issus du résultat de la fonction **AudioUtil.resample()** (qui est contenu dans la variable **reaud**), donc des fichiers audios ayant la meme dimension et encodés numériquement en les mettant tous sous forme **stéréo**. Tous nos audios qu'ils soient en **mono** ou **stéréo** seront tous transformés en **stéréo** au travers de la variable **self.channel**. Lorsque **self.channel** est égale a 2, nous sommes en stéréo. Lorsque c'est 1 nous sommes en mono.\n",
    "    \n",
    "    * **AudioUtil.pad_trunc(rechan, self.duration)** processe les résulats issus de **AudioUtil.rechannel()** au travers de la variable **rechan** en convertissant tous les audios pour qu'ils aient la meme durée. Vu que nos audios avaient déjà tous la meme durée, 00:10 secondes, cette étape n'est donc pas nécéssaire mais prévient des données de test dont les longueurs audios pourraient différer, ce qui causerait des problèmes de dimensions aux tenseurs. **self.duration** est la durée pour laquelle nous voulons que tous nos audios soient de meme durée, dans notre cas 10.000 milliseconds donc 10 seconds car les audios de la base volumineuse le sont tous. Cela nous donne par conséquent le meme résultat que **AudioUtil.rechannel()** puisqu'aucune transformation tangible a été nécessaire.\n",
    "    \n",
    "    * **AudioUtil.time_shift(dur_aud, self.shift_pct)** : Meme principe : on prend les résultats de la fonction précédente et on les processe. Ainsi on s'assure que l'on fait de manière séquentielle et que l'on oublie aucune étape de préprocessing. **dur_aud** est le résultat de la fonction **AudioUtil.pad_trunc()**. Par contre **self.shift_pct()** est le pourcentage pour lequel nous voulons shifter nos audios soit vers la gauche ou la droite pour créer de la diversité dans nos données et donc accroitre le pouvoir de généralisation de notre modèle. Elle sera égale a 0.4 soit 40% . C'est une valeur que nous avons pris juste de manière arbitraire. Vu qu'il doit etre décidé avant de faire tourner le modèle, c'est donc un **hyperparamètre**.\n",
    "    \n",
    "    * **AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)** : Prend les résultats de **AudioUtil.time_shift()** et les processe. L'idée est de transformer nos audios en spectograms donc en images, pour les faire passer au travers d'un CNN et trouver des relations intéressantes. **n_mels** est le nombre de bandes fréquentes en mels, donc la taille du spectogram. En réalité, les spectograms bruts issus des données audios ne sont pas chaque fois intéréssantes. En effet, souvent l'image générée par ce spectogram est très invisible, ce qui peut etre problématique pour notre modèle de DL.Comment reconnaitre comment se comporte un audio si son spectogram est illisible ? L'idée donc que les chercheurs ont eu est d'introduire ce que l'on appelle **spectogram de mel** d'ou le nom du paramètres **n_mels**. Le spectogram de mel est un spectogram qui, plutot que d'utiliser les fréquences des audios sur l'axe y et le temps sur l'axe x, utilise **l'échelle de mel** sur l'axe y et le temps sur l'axe x, ce qui donne des résultats très intéréssants et raisonnables.**n_fft** est la fenetre de temps. **n_mels** et **n_fft** étant définis a l'avance, ce sont des **hyperparamètres**\n",
    "    \n",
    "    Un article intéressant dont nous nous sommes servis pour comprendre l'échelle de mel est au lien suivant : https://fr.wikipedia.org/wiki/Échelle_des_mels.\n",
    "    \n",
    "    * **AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)** : Prend les résultats de la fonction **AudioUtil.spectro_gram()** qui est contenu dans **sgram** et le procèsse. A cette étape, nos audios sont sous forme de spectogram de mels. Mais vous voulons créer des **spectograms augmentés**.Tout comme le **time_shift()**, nous voulons créer de la diversité dans nos données et qui se traduiront par un modèle plus robuste et un bon pouvoir généralisateur. Cette augmentation se fait au travers des variables **n_freq_masks** ou masque de fréquence et de **n_time_masks** ou masque de temps. **n_freq_masks** masque au hasard une plage de fréquences consécutives en ajoutant des barres horizontales sur le spectrogramme tandis que **n_time_masks** similaire aux masques de fréquence,bloquent au hasard des plages de temps du spectrogramme en utilisant des barres verticales. **n_freq_masks** et **n_time_masks** étant définis avant de tourner le modèle, ce sont également des **hyperparamètres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "4aea3a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDS(Dataset):\n",
    "  def __init__(self, df, data_path):\n",
    "    self.df = df\n",
    "    self.data_path = str(data_path)\n",
    "    self.duration = 10000\n",
    "    self.sr = 44100\n",
    "    self.channel = 2\n",
    "    self.shift_pct = 0.4\n",
    "            \n",
    "  # ----------------------------\n",
    "  # Nombre d'éléments dans la base de donnée \n",
    "  # ----------------------------\n",
    "  def __len__(self):\n",
    "    return len(self.df)    \n",
    "    \n",
    "  # ----------------------------\n",
    "  # Avoir le ième élément \n",
    "  # ----------------------------\n",
    "  def __getitem__(self, idx):\n",
    "    # Chemin de fichier absolu du fichier audio - concaténez le répertoire audio avec\n",
    "    # le relative path\n",
    "    audio_file = self.data_path + self.df.loc[idx, 'relative_path']\n",
    "    # Obtenir l'ID de la Classe\n",
    "    class_id = self.df.loc[idx, 'hasbird']\n",
    "\n",
    "    aud = AudioUtil.open(audio_file)\n",
    "    # Certains sons ont un taux d'échantillonnage plus élevé ou moins de canaux par rapport a la\n",
    "    # majorité. Donc, faisons en sorte que tous les sons aient le même nombre de canaux et le même\n",
    "    # taux d'échantillonnage. À moins que la fréquence d'échantillonnage ne soit la même, le pad_trunc continuera a\n",
    "    # donner des tableaux de longueurs différentes, même si la durée du son est\n",
    "    # le même.\n",
    "    reaud = AudioUtil.resample(aud, self.sr)\n",
    "    rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "\n",
    "    dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "    shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "    sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "\n",
    "    return aug_sgram , class_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9096557d",
   "metadata": {},
   "source": [
    "> Nous passons maintenant dans notre classe SoundDS, le dataframe représentant la métadonnée et lien de répertoire vers le fichier volumineux contenant les fichiers audio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7acf8369",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sounds = SoundDS(df,'/Users/nacersere/Downloads/wav') # wav représente le fichier volumineux des audios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3155b3",
   "metadata": {},
   "source": [
    "> Comme nous pouvons le voir, my_sounds est un objet crée via une classe. Lorsque l'on essaie d'afficher cet objet, Pyton nous dit qu'il s'agit d'un objet de type SoundDS stocké quelque part dans la mémoire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d7b8a23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SoundDS at 0x29a8feb80>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sounds   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca26c225",
   "metadata": {},
   "source": [
    "> Lorsque nous utilisons la fonction len() sur notre objet de type SoundDS, il nous retourne un résultat or nous savons que la fonction len() fonctionne pour un certain type de données en Python, mais pourquoi fonctionne t'il sur un objet de type SoundDS, objet crée de manière artificielle et qui n'existait pas auparavant?. Cela est du a la surcharge des opérateurs. C'est la fonction $__len__()$ dans la classe SoundDS qui permet d'avoir ce résultat. On voit bien que l'on a 7690 données audios disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1667a47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7690"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_sounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35d09da",
   "metadata": {},
   "source": [
    "> Nous allons a présent essayer de voir ce qui se passe a l'intérieur de notre objet my_sounds. Tout comme avec la fonction len(), les $[]$ sont uilisés avec un certain type de données spécifiques en Python. Mais pourquoi un objet de type SoundDS fonctionne t'il avec les crochets? Cette propriété est du aussi a la surchage des opérateurs. Le fait d'étendre les opérateurs de base fondamentaux réservés pour un certain type de donnée a un autre type de donnée. Ainsi, grace a la fonction $__getitem__()$ de la classe SoundDS, nous pouvons voir le 8ième élément de nos données\n",
    "\n",
    "> En indexant notre objet my_sounds, nous avons comme résultat un tuple. Le premier élément du tuple est un tensor qui représente le **spectogram augmenté** obtenu lors de notre préprocessing sous forme numérique avec des chiffres et le second élément est le label de l'audio pour savoir si oui ou non il contient des cris d'oiseaux. 0 pour Non et 1 pour oui.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cfa4d635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 33.4620,  35.1140,  30.1704,  ...,  34.4985,  35.7045,  20.2986],\n",
       "          [ 34.2874,  33.0401,  35.6473,  ...,  29.0326,  33.0858,  26.6811],\n",
       "          [ 27.6349,  26.3337,  33.3903,  ...,  24.0278,  28.5904,  25.5299],\n",
       "          ...,\n",
       "          [-19.7180, -23.8581, -23.3542,  ..., -23.3197, -20.3997, -22.7305],\n",
       "          [-21.1493, -22.7330, -21.9534,  ..., -24.3419, -22.9597, -23.0070],\n",
       "          [-23.6225, -23.4210, -24.4804,  ..., -24.4417, -24.8352, -25.1972]],\n",
       " \n",
       "         [[ 33.4620,  35.1140,  30.1704,  ...,  34.4985,  35.7045,  20.2986],\n",
       "          [ 34.2874,  33.0401,  35.6473,  ...,  29.0326,  33.0858,  26.6811],\n",
       "          [ 27.6349,  26.3337,  33.3903,  ...,  24.0278,  28.5904,  25.5299],\n",
       "          ...,\n",
       "          [-19.7180, -23.8581, -23.3542,  ..., -23.3197, -20.3997, -22.7305],\n",
       "          [-21.1493, -22.7330, -21.9534,  ..., -24.3419, -22.9597, -23.0070],\n",
       "          [-23.6225, -23.4210, -24.4804,  ..., -24.4417, -24.8352, -25.1972]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sounds[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "676e7b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 31.8502,  32.7419,  24.4008,  ...,  36.4841,  34.1769,  26.6737],\n",
       "         [ 30.4974,  28.3375,  29.1493,  ...,  32.5962,  35.4485,  20.6440],\n",
       "         [ 31.3923,  31.3897,  29.0918,  ...,  24.9208,  30.0547,  18.2222],\n",
       "         ...,\n",
       "         [-22.6661, -23.2208, -23.0941,  ..., -22.4938, -22.7700, -22.2436],\n",
       "         [-25.6760, -23.9084, -22.4351,  ..., -21.3864, -22.6048, -23.2098],\n",
       "         [-24.4264, -23.9438, -23.3493,  ..., -23.4859, -24.1167, -24.6893]],\n",
       "\n",
       "        [[ 31.8502,  32.7419,  24.4008,  ...,  36.4841,  34.1769,  26.6737],\n",
       "         [ 30.4974,  28.3375,  29.1493,  ...,  32.5962,  35.4485,  20.6440],\n",
       "         [ 31.3923,  31.3897,  29.0918,  ...,  24.9208,  30.0547,  18.2222],\n",
       "         ...,\n",
       "         [-22.6661, -23.2208, -23.0941,  ..., -22.4938, -22.7700, -22.2436],\n",
       "         [-25.6760, -23.9084, -22.4351,  ..., -21.3864, -22.6048, -23.2098],\n",
       "         [-24.4264, -23.9438, -23.3493,  ..., -23.4859, -24.1167, -24.6893]]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sounds[9][0]  # 1er élément du tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c6d6fb93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sounds[9][1]   # second element du tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f343dffc",
   "metadata": {},
   "source": [
    "> On voit clairement que pour chaque fichier audio, nous avons le label correspondant qui va avec et le preprocessing qui a été effectué de manière **séquentielle** et non **linéaire**. Nous sommes enfin pret a passer a la modélisation : Le modèle de DL en Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809daeef",
   "metadata": {},
   "source": [
    "## MODÈLE CNN DE DEEP LEARNING AVEC PYTORCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7443a1",
   "metadata": {},
   "source": [
    "> Dans la suite de notre projet, nous avons adopté deux approches : \n",
    "\n",
    "- Nous allons spliter nos données en deux : 80% de données en entrainement et 20% en test, puis nous allons calculer sur chaque portion (entrainement et test), la **loss** et l'**accuracy** pour voir si notre modèle performe bien \n",
    "\n",
    "\n",
    "\n",
    "- Nous allons utiliser dans une seconde approche, une validation croisée pour confirmer les performances du modèle.\n",
    "\n",
    "\n",
    "> Voici comment nous procédons : \n",
    "\n",
    "- Nous prenons le nombre d'observations totales de nos données audio\n",
    "- Nous multiplions la taille de nos données par 0.8 en arrondissant, ce qui nous 80% des observations totales\n",
    "- Nous calculons la portion restante entre le nombre total des données et celui des 80% on obtient celui des 20% des observations totales \n",
    "- Nous utilisons l'équivalent de la fonction **train_test_split()** en **sklearn** sous Pytorch qui est **random_split**, tout en prenant soin de spécifier la graine du générateur de nombres aléatoires pour assurer la reproductibilité en le fixant a 42. Nous passons a cette fonction l'objet my_sounds et la portion des données d'entrainement et de test sous forme de liste en prenant soin de conserver l'ordre. \n",
    "- Pytorch nous retourne donc deux données : le jeu d'entrainement et le jeu de test.\n",
    "\n",
    "> Une fois les jeux de données d'entrainement et de test obtenus, nous les chargeons sous forme de **dataloader** en Pytorch. \n",
    "\n",
    "> Le DataLoader fait ce que vous pensez qu'il pourrait faire. Il aide à charger des données dans un modèle. Pour l'entrainement et pour l'inférence. Il transforme un grand ensemble de données ou dataset en un itérable Python de plus petits morceaux. Ces petits morceaux sont appelés batchs ou mini-batchs et peuvent être définis par le paramètre batch_size. Pourquoi faire ceci? Parce que c'est plus efficace en termes de calcul. Dans un monde idéal, nous pourrions effectuer le forward pass et le backward pass sur toutes nos données à la fois. Mais une fois que nous commencons à utiliser des ensembles de données très volumineux, à moins que nous ne disposions d'une puissance de calcul infinie, il est plus facile de les diviser en lots. Cela donne également à notre modèle plus de possibilités d'amélioration. Avec les mini-batchs (petites portions de données), la descente de gradient est effectuée plus souvent par époque (une fois par mini-lot plutôt qu'une fois par époque)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "21c0eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_items = len(my_sounds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(my_sounds, [num_train, num_val],generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7460483",
   "metadata": {},
   "source": [
    "> Une fois cette étape accomplie, nous allons définir maintenant le modèle de DL proprement dit en Pytorch. Cette classe sera appelée AudioClassifier. \n",
    "\n",
    "> Notre modèle consiste en l'utilisation d'un modèle CNN de 03 Blocks de convolution en uilisant une initialisation de Kaiming pour les poids. Cela a pour effet de faire converger plus rapidement le modèle. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d6df10",
   "metadata": {},
   "source": [
    "**Construction du modèle de Deep Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "492b7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassifier (nn.Module):\n",
    "    # ----------------------------\n",
    "    # Architecture du modèle\n",
    "    # ----------------------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # 1er Block de Convolution avec Relu et Batch Norm. Utilisation d'une Initialisatioin de  Kaiming\n",
    "        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        # Second Block de Convolution\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        # Second Block de Convolution \n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        # Second Block de Convolution\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "\n",
    "        # Classificateur linéaire \n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=64, out_features=10)\n",
    "\n",
    "       \n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    " \n",
    "    # ----------------------------\n",
    "    # Forward pass \n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Tourner les blocks convolutionnels\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Pool adaptatif et aplatir pour l'entrée dans la couche linéaire\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Couche linéaire \n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Résultat Final\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ef7b08",
   "metadata": {},
   "source": [
    "> Puis on fait les étapes classiques de création d'un modèle sous Pytorch : \n",
    "\n",
    "- On instancie le modèle\n",
    "- On configure le GPU\n",
    "- On exporte le modèle sous GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "22a1d5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myModel = AudioClassifier()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)\n",
    "next(myModel.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1704cc6a",
   "metadata": {},
   "source": [
    "> Récapitulatif du modèle de DL sous Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9c0c09c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of AudioClassifier(\n",
       "  (conv1): Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (relu1): ReLU()\n",
       "  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (relu3): ReLU()\n",
       "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (relu4): ReLU()\n",
       "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (ap): AdaptiveAvgPool2d(output_size=1)\n",
       "  (lin): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (10): ReLU()\n",
       "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myModel.parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc5b51f",
   "metadata": {},
   "source": [
    "> Vu que nous sommes dans un problème de classification, il est important lors du Forward pass et du Backward pass de calculer la **loss** et l'**accuracy**\n",
    "\n",
    "- Pour ce faire, nous avons écris des fonctions customisées permettant de calculer l'accuracy et la loss de notre modèle lors de chaque passage, donc de chaque epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc88506b",
   "metadata": {},
   "source": [
    "> Fonction customisée permettant de calculer l'accuracy de notre modèle de DL sous PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "acc6fa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()    # nous avons utilisé PyTorch et torch.eq()\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e5f57",
   "metadata": {},
   "source": [
    "> Fonction customisée permettant de calculer la loss de notre modèle de DL sous PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "176e2d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()          # nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=myModel.parameters(), # nous avons utilisé PyTorch et torch.optim.SGD()\n",
    "                            lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db644fdf",
   "metadata": {},
   "source": [
    "   *Nous avons utilisé un gradient stochastique car c'est avec lui que nous avons atteint la meilleure performance du modèle. Avec un learning rate de 0.1, nous avons performé très bien en termes de Loss et Accuracy. Avec un Adam et un learning rate de 0.01, nous avons fait tout aussi bien. La différence entre les deux modèles en termes de performance est de 1%. SGD surperforme Adam de 1%.  Tous les modèles ont été sauvegardés et seront fournis avec le notebook final.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7f6348",
   "metadata": {},
   "source": [
    "> L'étape la plus importante après avoir défini le modèle, la loss et l'accuracy, est de définir une fonction d'entrainement et de test PyTorch. L'avantage de fonctionaliser l'étape d'entrainement et de test PyTorch est que plutot que d'écrire les boucles d'entrainement et de test encore et encore, nous pouvons écrire une fonction en 1 seule fois et l'appeler a volonté pour entrainer/tester notre modèle. Voici comment nous avons procédé.\n",
    "\n",
    "- Nous définissons une fonction train_step qui prend en entrée :\n",
    "    * un modèle de type torch.nn.Module : notre modèle de DL dans de cas \n",
    "    * une donnée/data_loader de type torch.utils.data.DataLoader : nos données d'netrainement train_dl\n",
    "    * une fonction loss de type torch.nn.Module : notre fonction loss_fn customisée PyTorch\n",
    "    * une fontion accuracy : notre fonction customisée accuracy_fn PyTorch\n",
    "    * et une device de type torch.device : Notre GPU\n",
    "    \n",
    "- Pour la définition de la fonction test_step, la logique est la meme que celle de la fonction train_step\n",
    "\n",
    "- En écrivant nos fonctions nous avons privilégié des annotations de type. Les annotations de type Python permettent a l'utilisateur de savoir a l'avance en utilisant une fonction, quels sont les types des paramètres qui sont valides pour la fonction. Par exemple dans la fonction test_step, le modèle doit etre de type torch.nn.Module pour que la fonction puisse marcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7dc91883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # Envoyons X et y sur GPU\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calcul de la loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        train_acc += accuracy_fn(y_true=y,\n",
    "                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels\n",
    "\n",
    "        # 3. Optimisation zero_grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimisation step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calcul de la loss et accuracy par epoch et affichons ce qui se passe \n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "def test_step(data_loader: torch.utils.data.DataLoader,\n",
    "              model: torch.nn.Module,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              accuracy_fn,\n",
    "              device: torch.device = device):\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model.eval() # configurer le modèle en mode evaluation\n",
    "    # Configurons le torch inference context manager\n",
    "    with torch.inference_mode(): \n",
    "        for X, y in data_loader:\n",
    "            # Envoyons X et y sur GPU\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # 1. Forward pass\n",
    "            test_pred = model(X)\n",
    "            \n",
    "            # 2. Calcul de la loss et de  l'accuracy\n",
    "            test_loss += loss_fn(test_pred, y)\n",
    "            test_acc += accuracy_fn(y_true=y,\n",
    "                y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels\n",
    "            )\n",
    "    \n",
    "        # Adjustement des metrics and affichage\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader)\n",
    "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dfb784",
   "metadata": {},
   "source": [
    "> Après avoir défini des fonctions d'entrainement, nous pouvons passer a l'apprentissage proprement dit. Nous procédons comme suit : \n",
    "\n",
    "- Nous définissons le nombre d'épochs qui sera de 150.\n",
    "- Nous appelons les fonctions train_step & test_step en faisant passer les paramètres correspondants. \n",
    "- Nous laissons le modèle apprendre sur GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad36ab61",
   "metadata": {},
   "source": [
    "**Apprentisage du modèle avec PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "31216cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fdad8854554ec8a265299a83c72646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Train loss: 0.61031 | Train accuracy: 75.71%\n",
      "Test loss: 0.51395 | Test accuracy: 76.10%\n",
      "\n",
      "Epoch: 1\n",
      "---------\n",
      "Train loss: 0.49988 | Train accuracy: 77.22%\n",
      "Test loss: 0.65322 | Test accuracy: 60.70%\n",
      "\n",
      "Epoch: 2\n",
      "---------\n",
      "Train loss: 0.47145 | Train accuracy: 79.22%\n",
      "Test loss: 0.47678 | Test accuracy: 78.93%\n",
      "\n",
      "Epoch: 3\n",
      "---------\n",
      "Train loss: 0.45551 | Train accuracy: 80.11%\n",
      "Test loss: 0.45443 | Test accuracy: 79.96%\n",
      "\n",
      "Epoch: 4\n",
      "---------\n",
      "Train loss: 0.44304 | Train accuracy: 80.99%\n",
      "Test loss: 0.46260 | Test accuracy: 79.77%\n",
      "\n",
      "Epoch: 5\n",
      "---------\n",
      "Train loss: 0.42798 | Train accuracy: 82.09%\n",
      "Test loss: 0.42512 | Test accuracy: 81.57%\n",
      "\n",
      "Epoch: 6\n",
      "---------\n",
      "Train loss: 0.41745 | Train accuracy: 82.53%\n",
      "Test loss: 0.42919 | Test accuracy: 81.44%\n",
      "\n",
      "Epoch: 7\n",
      "---------\n",
      "Train loss: 0.40412 | Train accuracy: 83.43%\n",
      "Test loss: 0.49962 | Test accuracy: 80.22%\n",
      "\n",
      "Epoch: 8\n",
      "---------\n",
      "Train loss: 0.39393 | Train accuracy: 83.57%\n",
      "Test loss: 0.43037 | Test accuracy: 82.47%\n",
      "\n",
      "Epoch: 9\n",
      "---------\n",
      "Train loss: 0.38574 | Train accuracy: 84.38%\n",
      "Test loss: 0.39503 | Test accuracy: 83.96%\n",
      "\n",
      "Epoch: 10\n",
      "---------\n",
      "Train loss: 0.38206 | Train accuracy: 84.63%\n",
      "Test loss: 0.50228 | Test accuracy: 78.93%\n",
      "\n",
      "Epoch: 11\n",
      "---------\n",
      "Train loss: 0.37836 | Train accuracy: 84.95%\n",
      "Test loss: 0.37434 | Test accuracy: 84.28%\n",
      "\n",
      "Epoch: 12\n",
      "---------\n",
      "Train loss: 0.36199 | Train accuracy: 85.63%\n",
      "Test loss: 0.36954 | Test accuracy: 84.92%\n",
      "\n",
      "Epoch: 13\n",
      "---------\n",
      "Train loss: 0.35932 | Train accuracy: 85.99%\n",
      "Test loss: 0.37647 | Test accuracy: 84.34%\n",
      "\n",
      "Epoch: 14\n",
      "---------\n",
      "Train loss: 0.35358 | Train accuracy: 85.97%\n",
      "Test loss: 0.36179 | Test accuracy: 85.63%\n",
      "\n",
      "Epoch: 15\n",
      "---------\n",
      "Train loss: 0.35250 | Train accuracy: 86.17%\n",
      "Test loss: 0.36029 | Test accuracy: 85.70%\n",
      "\n",
      "Epoch: 16\n",
      "---------\n",
      "Train loss: 0.34350 | Train accuracy: 86.62%\n",
      "Test loss: 0.36435 | Test accuracy: 85.44%\n",
      "\n",
      "Epoch: 17\n",
      "---------\n",
      "Train loss: 0.34440 | Train accuracy: 86.62%\n",
      "Test loss: 0.37431 | Test accuracy: 85.18%\n",
      "\n",
      "Epoch: 18\n",
      "---------\n",
      "Train loss: 0.33625 | Train accuracy: 87.42%\n",
      "Test loss: 0.42661 | Test accuracy: 82.93%\n",
      "\n",
      "Epoch: 19\n",
      "---------\n",
      "Train loss: 0.33662 | Train accuracy: 87.06%\n",
      "Test loss: 0.40367 | Test accuracy: 84.99%\n",
      "\n",
      "Epoch: 20\n",
      "---------\n",
      "Train loss: 0.32842 | Train accuracy: 87.19%\n",
      "Test loss: 0.35258 | Test accuracy: 86.47%\n",
      "\n",
      "Epoch: 21\n",
      "---------\n",
      "Train loss: 0.33195 | Train accuracy: 87.26%\n",
      "Test loss: 0.39143 | Test accuracy: 85.24%\n",
      "\n",
      "Epoch: 22\n",
      "---------\n",
      "Train loss: 0.33329 | Train accuracy: 87.13%\n",
      "Test loss: 0.36850 | Test accuracy: 84.54%\n",
      "\n",
      "Epoch: 23\n",
      "---------\n",
      "Train loss: 0.32622 | Train accuracy: 87.65%\n",
      "Test loss: 0.35905 | Test accuracy: 86.73%\n",
      "\n",
      "Epoch: 24\n",
      "---------\n",
      "Train loss: 0.32667 | Train accuracy: 87.32%\n",
      "Test loss: 0.39539 | Test accuracy: 84.02%\n",
      "\n",
      "Epoch: 25\n",
      "---------\n",
      "Train loss: 0.31691 | Train accuracy: 87.92%\n",
      "Test loss: 0.37322 | Test accuracy: 85.70%\n",
      "\n",
      "Epoch: 26\n",
      "---------\n",
      "Train loss: 0.32372 | Train accuracy: 87.45%\n",
      "Test loss: 0.34186 | Test accuracy: 87.11%\n",
      "\n",
      "Epoch: 27\n",
      "---------\n",
      "Train loss: 0.31144 | Train accuracy: 88.20%\n",
      "Test loss: 0.38058 | Test accuracy: 86.02%\n",
      "\n",
      "Epoch: 28\n",
      "---------\n",
      "Train loss: 0.31053 | Train accuracy: 88.13%\n",
      "Test loss: 0.34386 | Test accuracy: 86.53%\n",
      "\n",
      "Epoch: 29\n",
      "---------\n",
      "Train loss: 0.31869 | Train accuracy: 87.95%\n",
      "Test loss: 0.37834 | Test accuracy: 84.99%\n",
      "\n",
      "Epoch: 30\n",
      "---------\n",
      "Train loss: 0.31235 | Train accuracy: 87.92%\n",
      "Test loss: 0.37320 | Test accuracy: 86.60%\n",
      "\n",
      "Epoch: 31\n",
      "---------\n",
      "Train loss: 0.30901 | Train accuracy: 88.46%\n",
      "Test loss: 0.34860 | Test accuracy: 86.47%\n",
      "\n",
      "Epoch: 32\n",
      "---------\n",
      "Train loss: 0.30598 | Train accuracy: 88.62%\n",
      "Test loss: 0.35289 | Test accuracy: 87.18%\n",
      "\n",
      "Epoch: 33\n",
      "---------\n",
      "Train loss: 0.30918 | Train accuracy: 88.44%\n",
      "Test loss: 0.47800 | Test accuracy: 82.35%\n",
      "\n",
      "Epoch: 34\n",
      "---------\n",
      "Train loss: 0.30226 | Train accuracy: 88.31%\n",
      "Test loss: 0.36466 | Test accuracy: 85.89%\n",
      "\n",
      "Epoch: 35\n",
      "---------\n",
      "Train loss: 0.30229 | Train accuracy: 88.57%\n",
      "Test loss: 0.34751 | Test accuracy: 86.53%\n",
      "\n",
      "Epoch: 36\n",
      "---------\n",
      "Train loss: 0.30620 | Train accuracy: 88.26%\n",
      "Test loss: 0.35607 | Test accuracy: 86.79%\n",
      "\n",
      "Epoch: 37\n",
      "---------\n",
      "Train loss: 0.30249 | Train accuracy: 88.57%\n",
      "Test loss: 0.33166 | Test accuracy: 86.60%\n",
      "\n",
      "Epoch: 38\n",
      "---------\n",
      "Train loss: 0.30259 | Train accuracy: 88.38%\n",
      "Test loss: 0.34729 | Test accuracy: 87.18%\n",
      "\n",
      "Epoch: 39\n",
      "---------\n",
      "Train loss: 0.30193 | Train accuracy: 88.52%\n",
      "Test loss: 0.50545 | Test accuracy: 76.48%\n",
      "\n",
      "Epoch: 40\n",
      "---------\n",
      "Train loss: 0.30730 | Train accuracy: 87.79%\n",
      "Test loss: 0.34412 | Test accuracy: 86.53%\n",
      "\n",
      "Epoch: 41\n",
      "---------\n",
      "Train loss: 0.29267 | Train accuracy: 88.98%\n",
      "Test loss: 0.43645 | Test accuracy: 83.96%\n",
      "\n",
      "Epoch: 42\n",
      "---------\n",
      "Train loss: 0.29108 | Train accuracy: 88.88%\n",
      "Test loss: 0.53883 | Test accuracy: 81.64%\n",
      "\n",
      "Epoch: 43\n",
      "---------\n",
      "Train loss: 0.29672 | Train accuracy: 88.93%\n",
      "Test loss: 0.36619 | Test accuracy: 86.08%\n",
      "\n",
      "Epoch: 44\n",
      "---------\n",
      "Train loss: 0.29861 | Train accuracy: 88.96%\n",
      "Test loss: 0.34268 | Test accuracy: 86.53%\n",
      "\n",
      "Epoch: 45\n",
      "---------\n",
      "Train loss: 0.29715 | Train accuracy: 88.78%\n",
      "Test loss: 0.35538 | Test accuracy: 86.28%\n",
      "\n",
      "Epoch: 46\n",
      "---------\n",
      "Train loss: 0.29123 | Train accuracy: 89.03%\n",
      "Test loss: 0.37234 | Test accuracy: 85.76%\n",
      "\n",
      "Epoch: 47\n",
      "---------\n",
      "Train loss: 0.28766 | Train accuracy: 89.51%\n",
      "Test loss: 0.35534 | Test accuracy: 86.92%\n",
      "\n",
      "Epoch: 48\n",
      "---------\n",
      "Train loss: 0.31154 | Train accuracy: 88.39%\n",
      "Test loss: 0.36019 | Test accuracy: 86.40%\n",
      "\n",
      "Epoch: 49\n",
      "---------\n",
      "Train loss: 0.29506 | Train accuracy: 89.09%\n",
      "Test loss: 0.36371 | Test accuracy: 86.02%\n",
      "\n",
      "Epoch: 50\n",
      "---------\n",
      "Train loss: 0.28662 | Train accuracy: 89.16%\n",
      "Test loss: 0.32269 | Test accuracy: 87.11%\n",
      "\n",
      "Epoch: 51\n",
      "---------\n",
      "Train loss: 0.28720 | Train accuracy: 89.20%\n",
      "Test loss: 0.39646 | Test accuracy: 85.12%\n",
      "\n",
      "Epoch: 52\n",
      "---------\n",
      "Train loss: 0.28524 | Train accuracy: 89.43%\n",
      "Test loss: 0.36258 | Test accuracy: 86.73%\n",
      "\n",
      "Epoch: 53\n",
      "---------\n",
      "Train loss: 0.28685 | Train accuracy: 89.03%\n",
      "Test loss: 0.33700 | Test accuracy: 87.50%\n",
      "\n",
      "Epoch: 54\n",
      "---------\n",
      "Train loss: 0.28032 | Train accuracy: 89.55%\n",
      "Test loss: 0.34660 | Test accuracy: 86.66%\n",
      "\n",
      "Epoch: 55\n",
      "---------\n",
      "Train loss: 0.27649 | Train accuracy: 89.48%\n",
      "Test loss: 0.40738 | Test accuracy: 85.95%\n",
      "\n",
      "Epoch: 56\n",
      "---------\n",
      "Train loss: 0.28882 | Train accuracy: 89.38%\n",
      "Test loss: 0.36432 | Test accuracy: 86.15%\n",
      "\n",
      "Epoch: 57\n",
      "---------\n",
      "Train loss: 0.28674 | Train accuracy: 89.11%\n",
      "Test loss: 0.65259 | Test accuracy: 73.26%\n",
      "\n",
      "Epoch: 58\n",
      "---------\n",
      "Train loss: 0.28582 | Train accuracy: 89.12%\n",
      "Test loss: 0.43128 | Test accuracy: 84.99%\n",
      "\n",
      "Epoch: 59\n",
      "---------\n",
      "Train loss: 0.28213 | Train accuracy: 89.46%\n",
      "Test loss: 0.36073 | Test accuracy: 86.92%\n",
      "\n",
      "Epoch: 60\n",
      "---------\n",
      "Train loss: 0.27423 | Train accuracy: 89.87%\n",
      "Test loss: 0.35526 | Test accuracy: 87.18%\n",
      "\n",
      "Epoch: 61\n",
      "---------\n",
      "Train loss: 0.27520 | Train accuracy: 90.08%\n",
      "Test loss: 0.34723 | Test accuracy: 87.50%\n",
      "\n",
      "Epoch: 62\n",
      "---------\n",
      "Train loss: 0.27791 | Train accuracy: 90.11%\n",
      "Test loss: 0.38138 | Test accuracy: 85.95%\n",
      "\n",
      "Epoch: 63\n",
      "---------\n",
      "Train loss: 0.27366 | Train accuracy: 89.69%\n",
      "Test loss: 0.37094 | Test accuracy: 86.08%\n",
      "\n",
      "Epoch: 64\n",
      "---------\n",
      "Train loss: 0.27539 | Train accuracy: 89.79%\n",
      "Test loss: 0.33542 | Test accuracy: 87.69%\n",
      "\n",
      "Epoch: 65\n",
      "---------\n",
      "Train loss: 0.27813 | Train accuracy: 89.63%\n",
      "Test loss: 0.36068 | Test accuracy: 87.44%\n",
      "\n",
      "Epoch: 66\n",
      "---------\n",
      "Train loss: 0.27054 | Train accuracy: 89.69%\n",
      "Test loss: 0.37582 | Test accuracy: 86.34%\n",
      "\n",
      "Epoch: 67\n",
      "---------\n",
      "Train loss: 0.27378 | Train accuracy: 89.84%\n",
      "Test loss: 0.39825 | Test accuracy: 84.92%\n",
      "\n",
      "Epoch: 68\n",
      "---------\n",
      "Train loss: 0.27571 | Train accuracy: 89.89%\n",
      "Test loss: 0.35507 | Test accuracy: 87.11%\n",
      "\n",
      "Epoch: 69\n",
      "---------\n",
      "Train loss: 0.27580 | Train accuracy: 89.69%\n",
      "Test loss: 0.38560 | Test accuracy: 87.37%\n",
      "\n",
      "Epoch: 70\n",
      "---------\n",
      "Train loss: 0.27223 | Train accuracy: 89.72%\n",
      "Test loss: 0.33236 | Test accuracy: 87.05%\n",
      "\n",
      "Epoch: 71\n",
      "---------\n",
      "Train loss: 0.27551 | Train accuracy: 89.72%\n",
      "Test loss: 0.34700 | Test accuracy: 87.76%\n",
      "\n",
      "Epoch: 72\n",
      "---------\n",
      "Train loss: 0.26687 | Train accuracy: 90.49%\n",
      "Test loss: 0.35323 | Test accuracy: 86.98%\n",
      "\n",
      "Epoch: 73\n",
      "---------\n",
      "Train loss: 0.26858 | Train accuracy: 90.21%\n",
      "Test loss: 0.36053 | Test accuracy: 87.31%\n",
      "\n",
      "Epoch: 74\n",
      "---------\n",
      "Train loss: 0.27304 | Train accuracy: 89.76%\n",
      "Test loss: 0.37511 | Test accuracy: 86.28%\n",
      "\n",
      "Epoch: 75\n",
      "---------\n",
      "Train loss: 0.26755 | Train accuracy: 90.21%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.39306 | Test accuracy: 86.34%\n",
      "\n",
      "Epoch: 76\n",
      "---------\n",
      "Train loss: 0.26824 | Train accuracy: 89.89%\n",
      "Test loss: 0.34518 | Test accuracy: 86.66%\n",
      "\n",
      "Epoch: 77\n",
      "---------\n",
      "Train loss: 0.26908 | Train accuracy: 90.71%\n",
      "Test loss: 0.35599 | Test accuracy: 86.86%\n",
      "\n",
      "Epoch: 78\n",
      "---------\n",
      "Train loss: 0.26698 | Train accuracy: 89.82%\n",
      "Test loss: 0.38408 | Test accuracy: 86.60%\n",
      "\n",
      "Epoch: 79\n",
      "---------\n",
      "Train loss: 0.26747 | Train accuracy: 90.21%\n",
      "Test loss: 0.37226 | Test accuracy: 86.34%\n",
      "\n",
      "Epoch: 80\n",
      "---------\n",
      "Train loss: 0.26650 | Train accuracy: 90.32%\n",
      "Test loss: 0.33192 | Test accuracy: 87.24%\n",
      "\n",
      "Epoch: 81\n",
      "---------\n",
      "Train loss: 0.26081 | Train accuracy: 90.36%\n",
      "Test loss: 0.34773 | Test accuracy: 87.82%\n",
      "\n",
      "Epoch: 82\n",
      "---------\n",
      "Train loss: 0.25939 | Train accuracy: 90.36%\n",
      "Test loss: 0.35220 | Test accuracy: 88.34%\n",
      "\n",
      "Epoch: 83\n",
      "---------\n",
      "Train loss: 0.26196 | Train accuracy: 90.65%\n",
      "Test loss: 0.35449 | Test accuracy: 87.56%\n",
      "\n",
      "Epoch: 84\n",
      "---------\n",
      "Train loss: 0.26247 | Train accuracy: 90.28%\n",
      "Test loss: 0.36707 | Test accuracy: 86.98%\n",
      "\n",
      "Epoch: 85\n",
      "---------\n",
      "Train loss: 0.26057 | Train accuracy: 90.34%\n",
      "Test loss: 0.35205 | Test accuracy: 87.50%\n",
      "\n",
      "Epoch: 86\n",
      "---------\n",
      "Train loss: 0.25833 | Train accuracy: 90.21%\n",
      "Test loss: 0.42788 | Test accuracy: 84.99%\n",
      "\n",
      "Epoch: 87\n",
      "---------\n",
      "Train loss: 0.26069 | Train accuracy: 90.00%\n",
      "Test loss: 0.35327 | Test accuracy: 86.86%\n",
      "\n",
      "Epoch: 88\n",
      "---------\n",
      "Train loss: 0.26937 | Train accuracy: 90.19%\n",
      "Test loss: 0.35705 | Test accuracy: 87.11%\n",
      "\n",
      "Epoch: 89\n",
      "---------\n",
      "Train loss: 0.26554 | Train accuracy: 90.37%\n",
      "Test loss: 0.35496 | Test accuracy: 87.24%\n",
      "\n",
      "Epoch: 90\n",
      "---------\n",
      "Train loss: 0.27968 | Train accuracy: 89.72%\n",
      "Test loss: 0.38136 | Test accuracy: 86.66%\n",
      "\n",
      "Epoch: 91\n",
      "---------\n",
      "Train loss: 0.26155 | Train accuracy: 90.29%\n",
      "Test loss: 0.37474 | Test accuracy: 87.82%\n",
      "\n",
      "Epoch: 92\n",
      "---------\n",
      "Train loss: 0.25694 | Train accuracy: 90.86%\n",
      "Test loss: 0.38719 | Test accuracy: 85.82%\n",
      "\n",
      "Epoch: 93\n",
      "---------\n",
      "Train loss: 0.26123 | Train accuracy: 89.87%\n",
      "Test loss: 0.36294 | Test accuracy: 86.86%\n",
      "\n",
      "Epoch: 94\n",
      "---------\n",
      "Train loss: 0.25472 | Train accuracy: 90.63%\n",
      "Test loss: 0.36675 | Test accuracy: 86.98%\n",
      "\n",
      "Epoch: 95\n",
      "---------\n",
      "Train loss: 0.25405 | Train accuracy: 90.71%\n",
      "Test loss: 0.36945 | Test accuracy: 87.82%\n",
      "\n",
      "Epoch: 96\n",
      "---------\n",
      "Train loss: 0.26411 | Train accuracy: 90.03%\n",
      "Test loss: 0.34990 | Test accuracy: 87.63%\n",
      "\n",
      "Epoch: 97\n",
      "---------\n",
      "Train loss: 0.25501 | Train accuracy: 90.81%\n",
      "Test loss: 0.36270 | Test accuracy: 86.79%\n",
      "\n",
      "Epoch: 98\n",
      "---------\n",
      "Train loss: 0.25522 | Train accuracy: 90.65%\n",
      "Test loss: 0.46350 | Test accuracy: 81.44%\n",
      "\n",
      "Epoch: 99\n",
      "---------\n",
      "Train loss: 0.26342 | Train accuracy: 89.89%\n",
      "Test loss: 0.35419 | Test accuracy: 87.95%\n",
      "\n",
      "Epoch: 100\n",
      "---------\n",
      "Train loss: 0.25234 | Train accuracy: 90.86%\n",
      "Test loss: 0.34932 | Test accuracy: 87.56%\n",
      "\n",
      "Epoch: 101\n",
      "---------\n",
      "Train loss: 0.25331 | Train accuracy: 90.80%\n",
      "Test loss: 0.35623 | Test accuracy: 87.37%\n",
      "\n",
      "Epoch: 102\n",
      "---------\n",
      "Train loss: 0.25419 | Train accuracy: 90.78%\n",
      "Test loss: 0.35895 | Test accuracy: 87.31%\n",
      "\n",
      "Epoch: 103\n",
      "---------\n",
      "Train loss: 0.25164 | Train accuracy: 90.86%\n",
      "Test loss: 0.36394 | Test accuracy: 86.92%\n",
      "\n",
      "Epoch: 104\n",
      "---------\n",
      "Train loss: 0.25624 | Train accuracy: 90.55%\n",
      "Test loss: 0.60581 | Test accuracy: 80.73%\n",
      "\n",
      "Epoch: 105\n",
      "---------\n",
      "Train loss: 0.24804 | Train accuracy: 90.96%\n",
      "Test loss: 0.38223 | Test accuracy: 86.28%\n",
      "\n",
      "Epoch: 106\n",
      "---------\n",
      "Train loss: 0.25483 | Train accuracy: 90.76%\n",
      "Test loss: 0.33755 | Test accuracy: 88.14%\n",
      "\n",
      "Epoch: 107\n",
      "---------\n",
      "Train loss: 0.25551 | Train accuracy: 90.44%\n",
      "Test loss: 0.35416 | Test accuracy: 87.05%\n",
      "\n",
      "Epoch: 108\n",
      "---------\n",
      "Train loss: 0.25181 | Train accuracy: 90.65%\n",
      "Test loss: 0.37783 | Test accuracy: 87.31%\n",
      "\n",
      "Epoch: 109\n",
      "---------\n",
      "Train loss: 0.24857 | Train accuracy: 91.06%\n",
      "Test loss: 0.40204 | Test accuracy: 86.15%\n",
      "\n",
      "Epoch: 110\n",
      "---------\n",
      "Train loss: 0.24260 | Train accuracy: 90.88%\n",
      "Test loss: 0.42418 | Test accuracy: 86.73%\n",
      "\n",
      "Epoch: 111\n",
      "---------\n",
      "Train loss: 0.25476 | Train accuracy: 90.70%\n",
      "Test loss: 0.34957 | Test accuracy: 87.44%\n",
      "\n",
      "Epoch: 112\n",
      "---------\n",
      "Train loss: 0.24612 | Train accuracy: 91.04%\n",
      "Test loss: 0.35764 | Test accuracy: 87.69%\n",
      "\n",
      "Epoch: 113\n",
      "---------\n",
      "Train loss: 0.25012 | Train accuracy: 90.76%\n",
      "Test loss: 0.34666 | Test accuracy: 88.92%\n",
      "\n",
      "Epoch: 114\n",
      "---------\n",
      "Train loss: 0.24466 | Train accuracy: 91.07%\n",
      "Test loss: 0.34653 | Test accuracy: 88.79%\n",
      "\n",
      "Epoch: 115\n",
      "---------\n",
      "Train loss: 0.24803 | Train accuracy: 90.99%\n",
      "Test loss: 0.37313 | Test accuracy: 87.18%\n",
      "\n",
      "Epoch: 116\n",
      "---------\n",
      "Train loss: 0.24820 | Train accuracy: 90.96%\n",
      "Test loss: 0.34864 | Test accuracy: 88.14%\n",
      "\n",
      "Epoch: 117\n",
      "---------\n",
      "Train loss: 0.24828 | Train accuracy: 90.91%\n",
      "Test loss: 0.36220 | Test accuracy: 86.92%\n",
      "\n",
      "Epoch: 118\n",
      "---------\n",
      "Train loss: 0.24466 | Train accuracy: 90.93%\n",
      "Test loss: 0.34862 | Test accuracy: 87.24%\n",
      "\n",
      "Epoch: 119\n",
      "---------\n",
      "Train loss: 0.24442 | Train accuracy: 91.14%\n",
      "Test loss: 0.36025 | Test accuracy: 87.24%\n",
      "\n",
      "Epoch: 120\n",
      "---------\n",
      "Train loss: 0.24378 | Train accuracy: 90.78%\n",
      "Test loss: 0.37767 | Test accuracy: 87.31%\n",
      "\n",
      "Epoch: 121\n",
      "---------\n",
      "Train loss: 0.24481 | Train accuracy: 90.97%\n",
      "Test loss: 0.36330 | Test accuracy: 87.95%\n",
      "\n",
      "Epoch: 122\n",
      "---------\n",
      "Train loss: 0.24489 | Train accuracy: 90.93%\n",
      "Test loss: 0.36014 | Test accuracy: 88.66%\n",
      "\n",
      "Epoch: 123\n",
      "---------\n",
      "Train loss: 0.24380 | Train accuracy: 91.19%\n",
      "Test loss: 0.36695 | Test accuracy: 87.37%\n",
      "\n",
      "Epoch: 124\n",
      "---------\n",
      "Train loss: 0.23671 | Train accuracy: 91.49%\n",
      "Test loss: 0.35200 | Test accuracy: 87.76%\n",
      "\n",
      "Epoch: 125\n",
      "---------\n",
      "Train loss: 0.24248 | Train accuracy: 91.20%\n",
      "Test loss: 0.36812 | Test accuracy: 87.63%\n",
      "\n",
      "Epoch: 126\n",
      "---------\n",
      "Train loss: 0.24674 | Train accuracy: 90.78%\n",
      "Test loss: 0.36051 | Test accuracy: 87.50%\n",
      "\n",
      "Epoch: 127\n",
      "---------\n",
      "Train loss: 0.24479 | Train accuracy: 90.96%\n",
      "Test loss: 0.35825 | Test accuracy: 88.08%\n",
      "\n",
      "Epoch: 128\n",
      "---------\n",
      "Train loss: 0.24291 | Train accuracy: 91.14%\n",
      "Test loss: 0.34044 | Test accuracy: 88.53%\n",
      "\n",
      "Epoch: 129\n",
      "---------\n",
      "Train loss: 0.31906 | Train accuracy: 88.33%\n",
      "Test loss: 0.47428 | Test accuracy: 79.70%\n",
      "\n",
      "Epoch: 130\n",
      "---------\n",
      "Train loss: 0.34571 | Train accuracy: 86.46%\n",
      "Test loss: 0.44007 | Test accuracy: 82.22%\n",
      "\n",
      "Epoch: 131\n",
      "---------\n",
      "Train loss: 0.30515 | Train accuracy: 87.99%\n",
      "Test loss: 0.41640 | Test accuracy: 84.02%\n",
      "\n",
      "Epoch: 132\n",
      "---------\n",
      "Train loss: 0.28585 | Train accuracy: 89.16%\n",
      "Test loss: 0.36772 | Test accuracy: 87.24%\n",
      "\n",
      "Epoch: 133\n",
      "---------\n",
      "Train loss: 0.28706 | Train accuracy: 89.07%\n",
      "Test loss: 0.37957 | Test accuracy: 86.34%\n",
      "\n",
      "Epoch: 134\n",
      "---------\n",
      "Train loss: 0.29779 | Train accuracy: 88.77%\n",
      "Test loss: 0.42120 | Test accuracy: 85.44%\n",
      "\n",
      "Epoch: 135\n",
      "---------\n",
      "Train loss: 0.27516 | Train accuracy: 89.40%\n",
      "Test loss: 0.44409 | Test accuracy: 85.89%\n",
      "\n",
      "Epoch: 136\n",
      "---------\n",
      "Train loss: 0.27503 | Train accuracy: 89.71%\n",
      "Test loss: 0.34380 | Test accuracy: 86.92%\n",
      "\n",
      "Epoch: 137\n",
      "---------\n",
      "Train loss: 0.27330 | Train accuracy: 89.68%\n",
      "Test loss: 0.37369 | Test accuracy: 86.34%\n",
      "\n",
      "Epoch: 138\n",
      "---------\n",
      "Train loss: 0.26704 | Train accuracy: 90.06%\n",
      "Test loss: 0.37654 | Test accuracy: 86.73%\n",
      "\n",
      "Epoch: 139\n",
      "---------\n",
      "Train loss: 0.26273 | Train accuracy: 90.06%\n",
      "Test loss: 0.35560 | Test accuracy: 87.05%\n",
      "\n",
      "Epoch: 140\n",
      "---------\n",
      "Train loss: 0.27043 | Train accuracy: 89.51%\n",
      "Test loss: 0.36211 | Test accuracy: 87.44%\n",
      "\n",
      "Epoch: 141\n",
      "---------\n",
      "Train loss: 0.26131 | Train accuracy: 90.19%\n",
      "Test loss: 0.38298 | Test accuracy: 86.98%\n",
      "\n",
      "Epoch: 142\n",
      "---------\n",
      "Train loss: 0.25587 | Train accuracy: 90.58%\n",
      "Test loss: 0.35245 | Test accuracy: 88.08%\n",
      "\n",
      "Epoch: 143\n",
      "---------\n",
      "Train loss: 0.26033 | Train accuracy: 90.18%\n",
      "Test loss: 0.37885 | Test accuracy: 86.34%\n",
      "\n",
      "Epoch: 144\n",
      "---------\n",
      "Train loss: 0.25641 | Train accuracy: 90.42%\n",
      "Test loss: 0.36328 | Test accuracy: 87.24%\n",
      "\n",
      "Epoch: 145\n",
      "---------\n",
      "Train loss: 0.26084 | Train accuracy: 90.70%\n",
      "Test loss: 0.43976 | Test accuracy: 84.60%\n",
      "\n",
      "Epoch: 146\n",
      "---------\n",
      "Train loss: 0.26250 | Train accuracy: 90.18%\n",
      "Test loss: 0.37769 | Test accuracy: 85.89%\n",
      "\n",
      "Epoch: 147\n",
      "---------\n",
      "Train loss: 0.26039 | Train accuracy: 90.42%\n",
      "Test loss: 0.33626 | Test accuracy: 87.69%\n",
      "\n",
      "Epoch: 148\n",
      "---------\n",
      "Train loss: 0.25481 | Train accuracy: 90.44%\n",
      "Test loss: 0.37846 | Test accuracy: 87.69%\n",
      "\n",
      "Epoch: 149\n",
      "---------\n",
      "Train loss: 0.25787 | Train accuracy: 90.50%\n",
      "Test loss: 0.36600 | Test accuracy: 86.92%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Mesure du temps\n",
    "from timeit import default_timer as timer\n",
    "train_time_start_on_gpu = timer()\n",
    "\n",
    "epochs = 150\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_step(data_loader=train_dl, \n",
    "        model=myModel, \n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )\n",
    "    test_step(data_loader=val_dl,\n",
    "        model=myModel,\n",
    "        loss_fn=loss_fn,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )\n",
    "\n",
    "train_time_end_on_gpu = timer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73c33f0",
   "metadata": {},
   "source": [
    "> Nous voyons que notre modèle performe très bien sur nos données. L'accuracy sur le jeu de test a chaque itération semble etre bon. Vérifions la performance globale de notre modèle. Pour cela, nous avons écris une fonction pour permet d'évaluer notre modèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "97211221",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn, \n",
    "               device: torch.device = device):\n",
    "    \"\"\"Evaluates a given model on a given dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "        device (str, optional): Target device to compute on. Defaults to device.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Envoyons X et y sur GPU\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
    "        \n",
    "        # Scale loss and acc\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "    return {\"model_name\": model.__class__.__name__, # marche seulement si le modèle a été crée avec une classe\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8269dc6",
   "metadata": {},
   "source": [
    "> Notre modèle performe **très bien** en terme de performance. En effet, ce dernier a atteint un accuracy de 87.95% sur le jeu de test. Pour un modèle de deep learning et un simple CNN, c'est **très acceptable**. Nous rappelons que nous avons utilisé une initialisation de Kaiming des poids et nos fonctions de préprocessing semblent bien marcher sur nos audios : spectogram augmenté obtenu via spectogram de mel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "97a21e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'AudioClassifier',\n",
       " 'model_loss': 0.3632675111293793,\n",
       " 'model_acc': 87.95103092783505}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_results = eval_model(model=myModel, data_loader=val_dl,\n",
    "    loss_fn=loss_fn, accuracy_fn=accuracy_fn,\n",
    "    device=device\n",
    ")\n",
    "my_model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea8c08",
   "metadata": {},
   "source": [
    "> Cependant, lorsqu'un modèle de DL fonctionne bien et meme très bien, c'est souvent le signe d'un overfitting ou le fait de surapprendre les données donc le bruit a l'intérieur, ce qui réduit drastiquement son pouvoir de généralisation. Pour confirmer les résultas, nous allons faire une validation croisée avec PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a84a8f8",
   "metadata": {},
   "source": [
    "**Cross Validation avec Pytorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4833d06",
   "metadata": {},
   "source": [
    "> Nous allons utiliser une validation croisée de deep learning pour voir si notre modèle performe vraiment bien. Pour ce faire nous allons utiliser ce qui est courant en deep learning pour faire du deep learning : Nous allons subdiviser nos données en : \n",
    "\n",
    "- Un jeu d'entrainement : 80% de la base de donnée \n",
    "- un jeu de validation pour hyperparametrer le modèle : 10% de la base de donnée \n",
    "- et un jeu de test pour la performance finale : 10% de la base de donnée \n",
    "\n",
    "> L'idée est simple : Nous allons entrainer et hyperparamétrer notre modèle sur le jeu d'entrainement et de validation. Nous allons faire apprendre notre modèle le plus longtemps possible sur nos données pour avoir de bons paramètres. Et nous allons tester son pouvoir de généralisation sur le jeu de test pour voir si notre modèle n'overfit pas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e181dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_items_cross = len(my_sounds)\n",
    "num_train_cross = round(num_items_cross * 0.8)   # entrainement\n",
    "num_val_cross = round(num_items_cross * 0.1)     # validation\n",
    "num_test_cross = round(num_items_cross * 0.1)    # test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92be9f24",
   "metadata": {},
   "source": [
    "> Nous utilisons comme précédemment la fonction random_split pour spliter nos données en jeu d'entrainement, en jeu de validation et en jeu de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8057f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_cross, val_ds_cross, test_ds_cross = random_split(my_sounds, [num_train_cross, num_val_cross, num_test_cross],generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b010b61",
   "metadata": {},
   "source": [
    "> On les transforme en DataLoader avec **torch.utils.data.DataLoader** : étape classique en Pytorch lorsque l'on fait du deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "52340145",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl_cross = torch.utils.data.DataLoader(train_ds_cross, batch_size=16, shuffle=True)  # entrainement\n",
    "val_dl_cross = torch.utils.data.DataLoader(val_ds_cross, batch_size=16, shuffle=False)   # validation\n",
    "test_dl_cross = torch.utils.data.DataLoader(test_ds_cross,batch_size=num_test_cross)   # test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507f510c",
   "metadata": {},
   "source": [
    "> On crée une fonction train_val_cross_step comme précédemment pour éviter d'écrire encore et encore du code pour faire apprendre le modèle de deep learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0c80f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_cross_step(model: torch.nn.Module,\n",
    "               train_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               val_loader:torch.utils.data.DataLoader,\n",
    "               device: torch.device = device):\n",
    "\n",
    "  # nombre d'epochs\n",
    "  epochs = 150\n",
    "\n",
    "\n",
    "  # initialisation des losses \n",
    "  train_loss      = torch.zeros(epochs)\n",
    "  val_loss        = torch.zeros(epochs)\n",
    "  train_accuracy  = torch.zeros(epochs)\n",
    "  val_accuraccy   = torch.zeros(epochs)\n",
    "\n",
    "\n",
    "  # Boucler a travers les epochs \n",
    "  for epochi in tqdm(range(epochs)):\n",
    "    \n",
    "    print(f\"Epoch: {epochi}\\n---------\")\n",
    "\n",
    "    # Boucler a travers  les training data batches\n",
    "    model.train() # configurer le modèle en mode entrainement \n",
    "    batch_loss = []\n",
    "    batch_accuracy  = []\n",
    "    for X,y in train_loader:\n",
    "\n",
    "      # Envoyer X et y sur le GPU\n",
    "      X = X.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      # forward pass et la loss\n",
    "      yHat = model(X)\n",
    "      loss = loss_fn(yHat,y)\n",
    "\n",
    "      # backprop\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # loss et accuracy a travers chaque batch\n",
    "      batch_loss.append(loss.item())\n",
    "      batch_accuracy.append( torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "    # fin de la boucle de chaque batch \n",
    "\n",
    "    # et obtention des loss moyennes et des accuracies sur toutes les batches\n",
    "    train_loss[epochi] = np.mean(batch_loss)\n",
    "    train_accuracy[epochi]  = 100*np.mean(batch_accuracy)\n",
    "    \n",
    "    \n",
    "\n",
    "    #### Performance sur le jeu de test (fait ici dans les batches!)\n",
    "    model.eval() # configuer le modèle en mode évaluation \n",
    "    batch_accuracy  = []\n",
    "    batch_loss = []\n",
    "    for X,y in val_loader:\n",
    "\n",
    "      # Envoyer X et y sur GPU\n",
    "      X = X.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      # forward pass et la loss\n",
    "      with torch.no_grad():\n",
    "        yHat = model(X)\n",
    "        loss = loss_fn(yHat,y)\n",
    "      \n",
    "      # loss et accuracy a travers chaque batch\n",
    "      batch_loss.append(loss.item())\n",
    "      batch_accuracy.append( torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "    # fin de la boucle de chaque batch \n",
    "\n",
    "    # et obtention des loss moyennes et des accuracies sur toutes les batches\n",
    "    val_loss[epochi] = np.mean(batch_loss)\n",
    "    val_accuraccy[epochi]  = 100*np.mean(batch_accuracy)\n",
    "\n",
    "    print(f\"Train loss: {train_loss[epochi]:.5f} | Train accuracy: {train_accuracy[epochi]:.2f}% | Validation loss: {val_loss[epochi]:.5f} | Validation accuracy: {val_accuraccy[epochi]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d092b29",
   "metadata": {},
   "source": [
    "> Nous pouvons entrainer maintenant le modèle sur la base d'entrainement et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "280a2863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fba315836c34ea9b4bb96004a66be26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Train loss: 0.61701 | Train accuracy: 75.24% | Validation loss: 0.52636 | Validation accuracy: 75.89\n",
      "Epoch: 1\n",
      "---------\n",
      "Train loss: 0.48487 | Train accuracy: 78.28% | Validation loss: 0.51428 | Validation accuracy: 77.42\n",
      "Epoch: 2\n",
      "---------\n",
      "Train loss: 0.46798 | Train accuracy: 79.55% | Validation loss: 0.49224 | Validation accuracy: 76.91\n",
      "Epoch: 3\n",
      "---------\n",
      "Train loss: 0.45574 | Train accuracy: 80.00% | Validation loss: 0.48620 | Validation accuracy: 77.30\n",
      "Epoch: 4\n",
      "---------\n",
      "Train loss: 0.44166 | Train accuracy: 80.65% | Validation loss: 0.49647 | Validation accuracy: 78.83\n",
      "Epoch: 5\n",
      "---------\n",
      "Train loss: 0.43643 | Train accuracy: 81.10% | Validation loss: 0.47622 | Validation accuracy: 79.34\n",
      "Epoch: 6\n",
      "---------\n",
      "Train loss: 0.42732 | Train accuracy: 82.18% | Validation loss: 0.44783 | Validation accuracy: 80.61\n",
      "Epoch: 7\n",
      "---------\n",
      "Train loss: 0.41603 | Train accuracy: 82.66% | Validation loss: 0.47604 | Validation accuracy: 79.72\n",
      "Epoch: 8\n",
      "---------\n",
      "Train loss: 0.41704 | Train accuracy: 82.63% | Validation loss: 0.45439 | Validation accuracy: 80.99\n",
      "Epoch: 9\n",
      "---------\n",
      "Train loss: 0.39993 | Train accuracy: 83.39% | Validation loss: 0.50998 | Validation accuracy: 77.81\n",
      "Epoch: 10\n",
      "---------\n",
      "Train loss: 0.40188 | Train accuracy: 83.26% | Validation loss: 0.42020 | Validation accuracy: 82.27\n",
      "Epoch: 11\n",
      "---------\n",
      "Train loss: 0.39342 | Train accuracy: 83.85% | Validation loss: 0.52293 | Validation accuracy: 75.89\n",
      "Epoch: 12\n",
      "---------\n",
      "Train loss: 0.38538 | Train accuracy: 84.38% | Validation loss: 0.44289 | Validation accuracy: 82.53\n",
      "Epoch: 13\n",
      "---------\n",
      "Train loss: 0.37928 | Train accuracy: 84.84% | Validation loss: 0.45922 | Validation accuracy: 80.99\n",
      "Epoch: 14\n",
      "---------\n",
      "Train loss: 0.36796 | Train accuracy: 85.37% | Validation loss: 0.43177 | Validation accuracy: 82.02\n",
      "Epoch: 15\n",
      "---------\n",
      "Train loss: 0.36131 | Train accuracy: 85.63% | Validation loss: 0.38765 | Validation accuracy: 84.57\n",
      "Epoch: 16\n",
      "---------\n",
      "Train loss: 0.36563 | Train accuracy: 85.16% | Validation loss: 0.49852 | Validation accuracy: 80.10\n",
      "Epoch: 17\n",
      "---------\n",
      "Train loss: 0.35340 | Train accuracy: 86.10% | Validation loss: 0.41792 | Validation accuracy: 83.29\n",
      "Epoch: 18\n",
      "---------\n",
      "Train loss: 0.35822 | Train accuracy: 85.73% | Validation loss: 0.40536 | Validation accuracy: 83.42\n",
      "Epoch: 19\n",
      "---------\n",
      "Train loss: 0.35666 | Train accuracy: 86.31% | Validation loss: 0.38144 | Validation accuracy: 85.08\n",
      "Epoch: 20\n",
      "---------\n",
      "Train loss: 0.34270 | Train accuracy: 86.51% | Validation loss: 0.38741 | Validation accuracy: 84.82\n",
      "Epoch: 21\n",
      "---------\n",
      "Train loss: 0.34487 | Train accuracy: 86.49% | Validation loss: 0.37738 | Validation accuracy: 84.06\n",
      "Epoch: 22\n",
      "---------\n",
      "Train loss: 0.34870 | Train accuracy: 86.35% | Validation loss: 0.37948 | Validation accuracy: 85.46\n",
      "Epoch: 23\n",
      "---------\n",
      "Train loss: 0.34306 | Train accuracy: 86.83% | Validation loss: 0.40779 | Validation accuracy: 85.59\n",
      "Epoch: 24\n",
      "---------\n",
      "Train loss: 0.34384 | Train accuracy: 86.30% | Validation loss: 0.41147 | Validation accuracy: 84.18\n",
      "Epoch: 25\n",
      "---------\n",
      "Train loss: 0.34013 | Train accuracy: 86.77% | Validation loss: 0.40349 | Validation accuracy: 84.95\n",
      "Epoch: 26\n",
      "---------\n",
      "Train loss: 0.33296 | Train accuracy: 86.98% | Validation loss: 0.38171 | Validation accuracy: 85.59\n",
      "Epoch: 27\n",
      "---------\n",
      "Train loss: 0.33657 | Train accuracy: 87.21% | Validation loss: 0.39365 | Validation accuracy: 84.82\n",
      "Epoch: 28\n",
      "---------\n",
      "Train loss: 0.33519 | Train accuracy: 86.88% | Validation loss: 0.37924 | Validation accuracy: 85.97\n",
      "Epoch: 29\n",
      "---------\n",
      "Train loss: 0.32887 | Train accuracy: 87.31% | Validation loss: 0.37990 | Validation accuracy: 85.71\n",
      "Epoch: 30\n",
      "---------\n",
      "Train loss: 0.31946 | Train accuracy: 87.52% | Validation loss: 0.38035 | Validation accuracy: 85.59\n",
      "Epoch: 31\n",
      "---------\n",
      "Train loss: 0.32719 | Train accuracy: 87.40% | Validation loss: 0.36184 | Validation accuracy: 85.97\n",
      "Epoch: 32\n",
      "---------\n",
      "Train loss: 0.32668 | Train accuracy: 87.66% | Validation loss: 0.38886 | Validation accuracy: 85.20\n",
      "Epoch: 33\n",
      "---------\n",
      "Train loss: 0.32798 | Train accuracy: 87.76% | Validation loss: 0.38799 | Validation accuracy: 85.33\n",
      "Epoch: 34\n",
      "---------\n",
      "Train loss: 0.32040 | Train accuracy: 87.76% | Validation loss: 0.36474 | Validation accuracy: 86.99\n",
      "Epoch: 35\n",
      "---------\n",
      "Train loss: 0.31871 | Train accuracy: 87.48% | Validation loss: 0.36177 | Validation accuracy: 85.97\n",
      "Epoch: 36\n",
      "---------\n",
      "Train loss: 0.32387 | Train accuracy: 87.79% | Validation loss: 0.40798 | Validation accuracy: 84.44\n",
      "Epoch: 37\n",
      "---------\n",
      "Train loss: 0.31714 | Train accuracy: 87.91% | Validation loss: 0.40275 | Validation accuracy: 84.82\n",
      "Epoch: 38\n",
      "---------\n",
      "Train loss: 0.31886 | Train accuracy: 87.92% | Validation loss: 0.36538 | Validation accuracy: 86.86\n",
      "Epoch: 39\n",
      "---------\n",
      "Train loss: 0.31149 | Train accuracy: 88.15% | Validation loss: 0.37787 | Validation accuracy: 85.59\n",
      "Epoch: 40\n",
      "---------\n",
      "Train loss: 0.31386 | Train accuracy: 87.68% | Validation loss: 0.36773 | Validation accuracy: 85.59\n",
      "Epoch: 41\n",
      "---------\n",
      "Train loss: 0.31655 | Train accuracy: 87.68% | Validation loss: 0.42767 | Validation accuracy: 84.06\n",
      "Epoch: 42\n",
      "---------\n",
      "Train loss: 0.31540 | Train accuracy: 87.68% | Validation loss: 0.39352 | Validation accuracy: 85.84\n",
      "Epoch: 43\n",
      "---------\n",
      "Train loss: 0.31730 | Train accuracy: 88.12% | Validation loss: 0.35675 | Validation accuracy: 86.22\n",
      "Epoch: 44\n",
      "---------\n",
      "Train loss: 0.31351 | Train accuracy: 88.07% | Validation loss: 0.35917 | Validation accuracy: 85.97\n",
      "Epoch: 45\n",
      "---------\n",
      "Train loss: 0.31295 | Train accuracy: 88.34% | Validation loss: 0.39443 | Validation accuracy: 85.46\n",
      "Epoch: 46\n",
      "---------\n",
      "Train loss: 0.31188 | Train accuracy: 88.07% | Validation loss: 0.35341 | Validation accuracy: 87.12\n",
      "Epoch: 47\n",
      "---------\n",
      "Train loss: 0.30738 | Train accuracy: 88.17% | Validation loss: 0.37260 | Validation accuracy: 86.22\n",
      "Epoch: 48\n",
      "---------\n",
      "Train loss: 0.30373 | Train accuracy: 88.05% | Validation loss: 0.37259 | Validation accuracy: 85.84\n",
      "Epoch: 49\n",
      "---------\n",
      "Train loss: 0.30414 | Train accuracy: 88.21% | Validation loss: 0.35994 | Validation accuracy: 86.73\n",
      "Epoch: 50\n",
      "---------\n",
      "Train loss: 0.30523 | Train accuracy: 88.62% | Validation loss: 0.35539 | Validation accuracy: 86.86\n",
      "Epoch: 51\n",
      "---------\n",
      "Train loss: 0.30874 | Train accuracy: 87.94% | Validation loss: 0.36693 | Validation accuracy: 86.48\n",
      "Epoch: 52\n",
      "---------\n",
      "Train loss: 0.30410 | Train accuracy: 87.91% | Validation loss: 0.38274 | Validation accuracy: 85.84\n",
      "Epoch: 53\n",
      "---------\n",
      "Train loss: 0.29990 | Train accuracy: 88.57% | Validation loss: 0.44229 | Validation accuracy: 82.53\n",
      "Epoch: 54\n",
      "---------\n",
      "Train loss: 0.29719 | Train accuracy: 88.52% | Validation loss: 0.35023 | Validation accuracy: 87.24\n",
      "Epoch: 55\n",
      "---------\n",
      "Train loss: 0.30415 | Train accuracy: 88.69% | Validation loss: 0.38874 | Validation accuracy: 86.10\n",
      "Epoch: 56\n",
      "---------\n",
      "Train loss: 0.29892 | Train accuracy: 88.54% | Validation loss: 0.40951 | Validation accuracy: 85.33\n",
      "Epoch: 57\n",
      "---------\n",
      "Train loss: 0.29844 | Train accuracy: 88.33% | Validation loss: 0.36065 | Validation accuracy: 86.22\n",
      "Epoch: 58\n",
      "---------\n",
      "Train loss: 0.29718 | Train accuracy: 88.72% | Validation loss: 0.35801 | Validation accuracy: 86.48\n",
      "Epoch: 59\n",
      "---------\n",
      "Train loss: 0.29768 | Train accuracy: 88.77% | Validation loss: 0.42548 | Validation accuracy: 84.06\n",
      "Epoch: 60\n",
      "---------\n",
      "Train loss: 0.29770 | Train accuracy: 88.23% | Validation loss: 0.37209 | Validation accuracy: 85.59\n",
      "Epoch: 61\n",
      "---------\n",
      "Train loss: 0.30386 | Train accuracy: 88.30% | Validation loss: 0.40512 | Validation accuracy: 84.18\n",
      "Epoch: 62\n",
      "---------\n",
      "Train loss: 0.30081 | Train accuracy: 88.54% | Validation loss: 0.37469 | Validation accuracy: 85.20\n",
      "Epoch: 63\n",
      "---------\n",
      "Train loss: 0.29296 | Train accuracy: 88.81% | Validation loss: 0.39155 | Validation accuracy: 85.33\n",
      "Epoch: 64\n",
      "---------\n",
      "Train loss: 0.29155 | Train accuracy: 88.98% | Validation loss: 0.37168 | Validation accuracy: 86.10\n",
      "Epoch: 65\n",
      "---------\n",
      "Train loss: 0.29436 | Train accuracy: 88.88% | Validation loss: 0.39231 | Validation accuracy: 85.46\n",
      "Epoch: 66\n",
      "---------\n",
      "Train loss: 0.29523 | Train accuracy: 88.99% | Validation loss: 0.40626 | Validation accuracy: 85.59\n",
      "Epoch: 67\n",
      "---------\n",
      "Train loss: 0.29992 | Train accuracy: 88.83% | Validation loss: 0.39079 | Validation accuracy: 84.82\n",
      "Epoch: 68\n",
      "---------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.29464 | Train accuracy: 88.83% | Validation loss: 0.35940 | Validation accuracy: 86.22\n",
      "Epoch: 69\n",
      "---------\n",
      "Train loss: 0.28697 | Train accuracy: 89.11% | Validation loss: 0.36028 | Validation accuracy: 85.33\n",
      "Epoch: 70\n",
      "---------\n",
      "Train loss: 0.28477 | Train accuracy: 89.40% | Validation loss: 0.35456 | Validation accuracy: 87.50\n",
      "Epoch: 71\n",
      "---------\n",
      "Train loss: 0.29156 | Train accuracy: 89.33% | Validation loss: 0.35021 | Validation accuracy: 87.24\n",
      "Epoch: 72\n",
      "---------\n",
      "Train loss: 0.28487 | Train accuracy: 89.42% | Validation loss: 0.41543 | Validation accuracy: 85.46\n",
      "Epoch: 73\n",
      "---------\n",
      "Train loss: 0.28235 | Train accuracy: 89.25% | Validation loss: 0.40039 | Validation accuracy: 86.73\n",
      "Epoch: 74\n",
      "---------\n",
      "Train loss: 0.29762 | Train accuracy: 88.86% | Validation loss: 0.38882 | Validation accuracy: 86.10\n",
      "Epoch: 75\n",
      "---------\n",
      "Train loss: 0.28334 | Train accuracy: 89.69% | Validation loss: 0.36950 | Validation accuracy: 86.35\n",
      "Epoch: 76\n",
      "---------\n",
      "Train loss: 0.28251 | Train accuracy: 89.24% | Validation loss: 0.41102 | Validation accuracy: 84.06\n",
      "Epoch: 77\n",
      "---------\n",
      "Train loss: 0.28262 | Train accuracy: 89.38% | Validation loss: 0.39692 | Validation accuracy: 85.46\n",
      "Epoch: 78\n",
      "---------\n",
      "Train loss: 0.29619 | Train accuracy: 88.75% | Validation loss: 0.35343 | Validation accuracy: 87.37\n",
      "Epoch: 79\n",
      "---------\n",
      "Train loss: 0.27686 | Train accuracy: 89.43% | Validation loss: 0.38367 | Validation accuracy: 85.71\n",
      "Epoch: 80\n",
      "---------\n",
      "Train loss: 0.28418 | Train accuracy: 89.46% | Validation loss: 0.35992 | Validation accuracy: 86.61\n",
      "Epoch: 81\n",
      "---------\n",
      "Train loss: 0.28225 | Train accuracy: 89.33% | Validation loss: 0.34679 | Validation accuracy: 87.24\n",
      "Epoch: 82\n",
      "---------\n",
      "Train loss: 0.27921 | Train accuracy: 89.90% | Validation loss: 0.40621 | Validation accuracy: 85.97\n",
      "Epoch: 83\n",
      "---------\n",
      "Train loss: 0.28377 | Train accuracy: 89.14% | Validation loss: 0.35637 | Validation accuracy: 86.10\n",
      "Epoch: 84\n",
      "---------\n",
      "Train loss: 0.27885 | Train accuracy: 89.72% | Validation loss: 0.35526 | Validation accuracy: 86.86\n",
      "Epoch: 85\n",
      "---------\n",
      "Train loss: 0.28292 | Train accuracy: 89.43% | Validation loss: 0.35150 | Validation accuracy: 87.24\n",
      "Epoch: 86\n",
      "---------\n",
      "Train loss: 0.27396 | Train accuracy: 89.85% | Validation loss: 0.35595 | Validation accuracy: 85.33\n",
      "Epoch: 87\n",
      "---------\n",
      "Train loss: 0.27837 | Train accuracy: 89.19% | Validation loss: 0.34473 | Validation accuracy: 87.37\n",
      "Epoch: 88\n",
      "---------\n",
      "Train loss: 0.27053 | Train accuracy: 89.71% | Validation loss: 0.39198 | Validation accuracy: 86.10\n",
      "Epoch: 89\n",
      "---------\n",
      "Train loss: 0.27641 | Train accuracy: 89.63% | Validation loss: 0.43039 | Validation accuracy: 84.31\n",
      "Epoch: 90\n",
      "---------\n",
      "Train loss: 0.28018 | Train accuracy: 89.51% | Validation loss: 0.35993 | Validation accuracy: 86.48\n",
      "Epoch: 91\n",
      "---------\n",
      "Train loss: 0.26874 | Train accuracy: 90.32% | Validation loss: 0.37062 | Validation accuracy: 86.61\n",
      "Epoch: 92\n",
      "---------\n",
      "Train loss: 0.28027 | Train accuracy: 89.30% | Validation loss: 0.38043 | Validation accuracy: 85.33\n",
      "Epoch: 93\n",
      "---------\n",
      "Train loss: 0.27559 | Train accuracy: 89.58% | Validation loss: 0.35823 | Validation accuracy: 86.35\n",
      "Epoch: 94\n",
      "---------\n",
      "Train loss: 0.27368 | Train accuracy: 89.30% | Validation loss: 0.36459 | Validation accuracy: 85.71\n",
      "Epoch: 95\n",
      "---------\n",
      "Train loss: 0.27645 | Train accuracy: 89.69% | Validation loss: 0.36728 | Validation accuracy: 85.84\n",
      "Epoch: 96\n",
      "---------\n",
      "Train loss: 0.27346 | Train accuracy: 89.76% | Validation loss: 0.35340 | Validation accuracy: 86.86\n",
      "Epoch: 97\n",
      "---------\n",
      "Train loss: 0.26914 | Train accuracy: 90.03% | Validation loss: 0.35934 | Validation accuracy: 86.86\n",
      "Epoch: 98\n",
      "---------\n",
      "Train loss: 0.27325 | Train accuracy: 89.98% | Validation loss: 0.39532 | Validation accuracy: 85.97\n",
      "Epoch: 99\n",
      "---------\n",
      "Train loss: 0.27118 | Train accuracy: 89.64% | Validation loss: 0.40177 | Validation accuracy: 86.10\n",
      "Epoch: 100\n",
      "---------\n",
      "Train loss: 0.28187 | Train accuracy: 89.42% | Validation loss: 0.42631 | Validation accuracy: 84.69\n",
      "Epoch: 101\n",
      "---------\n",
      "Train loss: 0.27707 | Train accuracy: 89.35% | Validation loss: 0.37763 | Validation accuracy: 85.46\n",
      "Epoch: 102\n",
      "---------\n",
      "Train loss: 0.26650 | Train accuracy: 89.69% | Validation loss: 0.37819 | Validation accuracy: 85.59\n",
      "Epoch: 103\n",
      "---------\n",
      "Train loss: 0.27552 | Train accuracy: 89.37% | Validation loss: 0.37492 | Validation accuracy: 85.97\n",
      "Epoch: 104\n",
      "---------\n",
      "Train loss: 0.26829 | Train accuracy: 90.11% | Validation loss: 0.39100 | Validation accuracy: 85.59\n",
      "Epoch: 105\n",
      "---------\n",
      "Train loss: 0.26836 | Train accuracy: 89.82% | Validation loss: 0.37504 | Validation accuracy: 86.22\n",
      "Epoch: 106\n",
      "---------\n",
      "Train loss: 0.27305 | Train accuracy: 89.59% | Validation loss: 0.37665 | Validation accuracy: 86.99\n",
      "Epoch: 107\n",
      "---------\n",
      "Train loss: 0.26471 | Train accuracy: 89.97% | Validation loss: 0.39007 | Validation accuracy: 85.46\n",
      "Epoch: 108\n",
      "---------\n",
      "Train loss: 0.26694 | Train accuracy: 89.82% | Validation loss: 0.36415 | Validation accuracy: 85.84\n",
      "Epoch: 109\n",
      "---------\n",
      "Train loss: 0.25881 | Train accuracy: 90.08% | Validation loss: 0.39811 | Validation accuracy: 86.86\n",
      "Epoch: 110\n",
      "---------\n",
      "Train loss: 0.27297 | Train accuracy: 89.76% | Validation loss: 0.40762 | Validation accuracy: 84.95\n",
      "Epoch: 111\n",
      "---------\n",
      "Train loss: 0.27024 | Train accuracy: 89.72% | Validation loss: 0.35611 | Validation accuracy: 86.73\n",
      "Epoch: 112\n",
      "---------\n",
      "Train loss: 0.25360 | Train accuracy: 90.36% | Validation loss: 0.36421 | Validation accuracy: 86.48\n",
      "Epoch: 113\n",
      "---------\n",
      "Train loss: 0.26300 | Train accuracy: 90.44% | Validation loss: 0.37271 | Validation accuracy: 85.84\n",
      "Epoch: 114\n",
      "---------\n",
      "Train loss: 0.26429 | Train accuracy: 90.19% | Validation loss: 0.32934 | Validation accuracy: 87.37\n",
      "Epoch: 115\n",
      "---------\n",
      "Train loss: 0.26805 | Train accuracy: 90.28% | Validation loss: 0.36510 | Validation accuracy: 87.12\n",
      "Epoch: 116\n",
      "---------\n",
      "Train loss: 0.26569 | Train accuracy: 89.81% | Validation loss: 0.36044 | Validation accuracy: 88.27\n",
      "Epoch: 117\n",
      "---------\n",
      "Train loss: 0.26288 | Train accuracy: 90.03% | Validation loss: 0.37293 | Validation accuracy: 87.37\n",
      "Epoch: 118\n",
      "---------\n",
      "Train loss: 0.26784 | Train accuracy: 90.23% | Validation loss: 0.38957 | Validation accuracy: 86.73\n",
      "Epoch: 119\n",
      "---------\n",
      "Train loss: 0.25979 | Train accuracy: 89.82% | Validation loss: 0.37666 | Validation accuracy: 87.37\n",
      "Epoch: 120\n",
      "---------\n",
      "Train loss: 0.25881 | Train accuracy: 90.71% | Validation loss: 0.43971 | Validation accuracy: 84.18\n",
      "Epoch: 121\n",
      "---------\n",
      "Train loss: 0.25727 | Train accuracy: 90.31% | Validation loss: 0.38645 | Validation accuracy: 86.48\n",
      "Epoch: 122\n",
      "---------\n",
      "Train loss: 0.26436 | Train accuracy: 89.92% | Validation loss: 0.37623 | Validation accuracy: 87.63\n",
      "Epoch: 123\n",
      "---------\n",
      "Train loss: 0.25511 | Train accuracy: 90.26% | Validation loss: 0.36225 | Validation accuracy: 85.97\n",
      "Epoch: 124\n",
      "---------\n",
      "Train loss: 0.25644 | Train accuracy: 90.37% | Validation loss: 0.35163 | Validation accuracy: 86.35\n",
      "Epoch: 125\n",
      "---------\n",
      "Train loss: 0.25614 | Train accuracy: 90.19% | Validation loss: 0.36708 | Validation accuracy: 85.97\n",
      "Epoch: 126\n",
      "---------\n",
      "Train loss: 0.25973 | Train accuracy: 90.44% | Validation loss: 0.37961 | Validation accuracy: 86.22\n",
      "Epoch: 127\n",
      "---------\n",
      "Train loss: 0.25384 | Train accuracy: 90.62% | Validation loss: 0.38209 | Validation accuracy: 86.48\n",
      "Epoch: 128\n",
      "---------\n",
      "Train loss: 0.26811 | Train accuracy: 89.87% | Validation loss: 0.38207 | Validation accuracy: 85.59\n",
      "Epoch: 129\n",
      "---------\n",
      "Train loss: 0.24880 | Train accuracy: 90.67% | Validation loss: 0.39782 | Validation accuracy: 85.33\n",
      "Epoch: 130\n",
      "---------\n",
      "Train loss: 0.25380 | Train accuracy: 90.37% | Validation loss: 0.39288 | Validation accuracy: 85.71\n",
      "Epoch: 131\n",
      "---------\n",
      "Train loss: 0.26745 | Train accuracy: 90.00% | Validation loss: 0.37385 | Validation accuracy: 86.99\n",
      "Epoch: 132\n",
      "---------\n",
      "Train loss: 0.25448 | Train accuracy: 90.49% | Validation loss: 0.38662 | Validation accuracy: 87.24\n",
      "Epoch: 133\n",
      "---------\n",
      "Train loss: 0.26615 | Train accuracy: 90.05% | Validation loss: 0.38063 | Validation accuracy: 86.86\n",
      "Epoch: 134\n",
      "---------\n",
      "Train loss: 0.24992 | Train accuracy: 90.60% | Validation loss: 0.42721 | Validation accuracy: 85.08\n",
      "Epoch: 135\n",
      "---------\n",
      "Train loss: 0.25335 | Train accuracy: 90.88% | Validation loss: 0.40113 | Validation accuracy: 84.69\n",
      "Epoch: 136\n",
      "---------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.25538 | Train accuracy: 90.58% | Validation loss: 0.36716 | Validation accuracy: 86.61\n",
      "Epoch: 137\n",
      "---------\n",
      "Train loss: 0.24966 | Train accuracy: 90.36% | Validation loss: 0.39819 | Validation accuracy: 86.22\n",
      "Epoch: 138\n",
      "---------\n",
      "Train loss: 0.25518 | Train accuracy: 90.41% | Validation loss: 0.39678 | Validation accuracy: 86.73\n",
      "Epoch: 139\n",
      "---------\n",
      "Train loss: 0.25494 | Train accuracy: 90.16% | Validation loss: 0.38438 | Validation accuracy: 85.59\n",
      "Epoch: 140\n",
      "---------\n",
      "Train loss: 0.25102 | Train accuracy: 90.65% | Validation loss: 0.36299 | Validation accuracy: 87.37\n",
      "Epoch: 141\n",
      "---------\n",
      "Train loss: 0.25458 | Train accuracy: 90.32% | Validation loss: 0.40253 | Validation accuracy: 85.97\n",
      "Epoch: 142\n",
      "---------\n",
      "Train loss: 0.25950 | Train accuracy: 90.29% | Validation loss: 0.37908 | Validation accuracy: 86.73\n",
      "Epoch: 143\n",
      "---------\n",
      "Train loss: 0.25158 | Train accuracy: 90.71% | Validation loss: 0.47466 | Validation accuracy: 84.82\n",
      "Epoch: 144\n",
      "---------\n",
      "Train loss: 0.26024 | Train accuracy: 90.41% | Validation loss: 0.38857 | Validation accuracy: 85.33\n",
      "Epoch: 145\n",
      "---------\n",
      "Train loss: 0.24975 | Train accuracy: 90.50% | Validation loss: 0.38508 | Validation accuracy: 86.61\n",
      "Epoch: 146\n",
      "---------\n",
      "Train loss: 0.24818 | Train accuracy: 90.32% | Validation loss: 0.38007 | Validation accuracy: 85.97\n",
      "Epoch: 147\n",
      "---------\n",
      "Train loss: 0.25866 | Train accuracy: 90.47% | Validation loss: 0.37201 | Validation accuracy: 87.63\n",
      "Epoch: 148\n",
      "---------\n",
      "Train loss: 0.24943 | Train accuracy: 90.41% | Validation loss: 0.40466 | Validation accuracy: 85.71\n",
      "Epoch: 149\n",
      "---------\n",
      "Train loss: 0.25603 | Train accuracy: 90.28% | Validation loss: 0.38701 | Validation accuracy: 85.84\n"
     ]
    }
   ],
   "source": [
    "train_val_cross_step(myModel,train_dl_cross,loss_fn,optimizer,val_dl_cross,'mps')   # apprentissage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1310a568",
   "metadata": {},
   "source": [
    "> Évaluons maintenant notre modèle sur la base de test pour voir la performance après que ce dernier aie appris sur le jeu d'entrainement et le jeu de validation\n",
    "\n",
    "> Nous le faisons au travers d'une fonction customisée comme auparavant permettant de calculer la loss et l'accuracy sur la base de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "973e07a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cross_step(model: torch.nn.Module,\n",
    "                   test_loader: torch.utils.data.DataLoader,\n",
    "                   loss_fn: torch.nn.Module,\n",
    "                   device: torch.device=device):\n",
    "    \n",
    "    model.eval()\n",
    "    X,y = next(iter(test_loader))\n",
    "    \n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        yHat = model(X)\n",
    "        loss = loss_fn(yHat,y)\n",
    "    \n",
    "    test_loss = loss.item()\n",
    "    test_accuracy = 100 * torch.mean((torch.argmax(yHat,axis=1) == y).float()).item()\n",
    "    \n",
    "    print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "54cde7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.32197 | Test accuracy: 89.47%\n"
     ]
    }
   ],
   "source": [
    "test_cross_step(myModel,test_dl_cross,loss_fn,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6110b881",
   "metadata": {},
   "source": [
    "> Notre modèle de deep learning performe **très très bien**. Comme nous pouvons le voir, la performance atteinte sur la base de test est de **89.47%** avec une cross-validation ! Par conséquent, notre modèle de DL n'overfit pas et apprend parfaitement bien les données. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9a6141",
   "metadata": {},
   "source": [
    "## PRÉDICTIONS DES AUDIOS DU JEU DE TEST A TELECHARGER SUR LE SITE WEB DU PROJET AVEC NOTRE MODÈLE DE DL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ecfad",
   "metadata": {},
   "source": [
    "> Nous allons maintenant importer la métadonnée du jeu de test a télécharger sur le site et essayer de prédire les audios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "2442a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_predict = pd.read_csv('https://ia903201.us.archive.org/28/items/birdaudiodetectionchallenge_test/badch_testset_blankresults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "7bd98e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>hasbird</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016765-bad0-4e2c-ad4e</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0005ae67-efdc-446f-aeee</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a3cad-ef99-4e5e-9845</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00270340-40d5-4947-9255</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002d0637-e8fb-44a8-916b</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    itemid  hasbird\n",
       "0  00016765-bad0-4e2c-ad4e      NaN\n",
       "1  0005ae67-efdc-446f-aeee      NaN\n",
       "2  000a3cad-ef99-4e5e-9845      NaN\n",
       "3  00270340-40d5-4947-9255      NaN\n",
       "4  002d0637-e8fb-44a8-916b      NaN"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_predict.head()   # voyons les premières lignes de la base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "9a61780a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>hasbird</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8615</th>\n",
       "      <td>ffe73a17-84eb-4fe8-99e2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8616</th>\n",
       "      <td>fff2dd8e-6973-4a62-86fa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8617</th>\n",
       "      <td>fff8ec92-afc0-48dd-9bb9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8618</th>\n",
       "      <td>fff92255-4d84-4136-9c58</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8619</th>\n",
       "      <td>fffc94c1-ff49-4860-b96e</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       itemid  hasbird\n",
       "8615  ffe73a17-84eb-4fe8-99e2      NaN\n",
       "8616  fff2dd8e-6973-4a62-86fa      NaN\n",
       "8617  fff8ec92-afc0-48dd-9bb9      NaN\n",
       "8618  fff92255-4d84-4136-9c58      NaN\n",
       "8619  fffc94c1-ff49-4860-b96e      NaN"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_predict.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a6ebfd",
   "metadata": {},
   "source": [
    "> Comme nous le voyons la métadonnée du jeu de test sur le site web ne contient aucun label 'hasbird'. Pas de 0 ou de 1. Nous allons assayer de prédire ces labels du jeu de test avec notre modèle de DL. \n",
    "\n",
    "> Comme précedemment, nous allons essayer de référencer les audios de la base de donnée de test téléchargée sur le site via les liens de répertoire. C'est exactement la meme procédure que l'on a fait précédemment pour entrainer nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "bc265301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "itemid      object\n",
       "hasbird    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_predict.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "298acf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_predict['itemid'] = df_test_predict['itemid'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "de7a97f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_predict['relative_path'] = '/' + df_test_predict['itemid'] + '.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "03e06db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>hasbird</th>\n",
       "      <th>relative_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016765-bad0-4e2c-ad4e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/00016765-bad0-4e2c-ad4e.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0005ae67-efdc-446f-aeee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/0005ae67-efdc-446f-aeee.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a3cad-ef99-4e5e-9845</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/000a3cad-ef99-4e5e-9845.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00270340-40d5-4947-9255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/00270340-40d5-4947-9255.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002d0637-e8fb-44a8-916b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/002d0637-e8fb-44a8-916b.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    itemid  hasbird                 relative_path\n",
       "0  00016765-bad0-4e2c-ad4e      NaN  /00016765-bad0-4e2c-ad4e.wav\n",
       "1  0005ae67-efdc-446f-aeee      NaN  /0005ae67-efdc-446f-aeee.wav\n",
       "2  000a3cad-ef99-4e5e-9845      NaN  /000a3cad-ef99-4e5e-9845.wav\n",
       "3  00270340-40d5-4947-9255      NaN  /00270340-40d5-4947-9255.wav\n",
       "4  002d0637-e8fb-44a8-916b      NaN  /002d0637-e8fb-44a8-916b.wav"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "fd278d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_predict = df_test_predict[['relative_path']] # comme il n'ya pas de label, nous retenons seulement le 'relative_path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "0f3ae3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sounds_predict = SoundDS(df_test_predict,'/Users/nacersere/Downloads/wav-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "054f5264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8620"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_sounds_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "4f8ad4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[12.3819,  2.1034,  5.4741,  ..., -0.0576, -6.2201, 10.9702],\n",
       "         [12.8517, -2.8655,  2.6813,  ..., -0.7263, -0.8349, 11.1678],\n",
       "         [14.0566, -0.4229,  5.5248,  ..., -2.2426,  0.9876,  9.5981],\n",
       "         ...,\n",
       "         [22.5322, 24.9996, 24.0414,  ..., 25.7002, 24.1045, 25.0527],\n",
       "         [24.7540, 24.6198, 25.9706,  ..., 25.5673, 25.5841, 25.7302],\n",
       "         [17.8433, 17.3420, 18.4870,  ..., 17.4362, 19.4744, 19.8112]],\n",
       "\n",
       "        [[12.3819,  2.1034,  5.4741,  ..., -0.0576, -6.2201, 10.9702],\n",
       "         [12.8517, -2.8655,  2.6813,  ..., -0.7263, -0.8349, 11.1678],\n",
       "         [14.0566, -0.4229,  5.5248,  ..., -2.2426,  0.9876,  9.5981],\n",
       "         ...,\n",
       "         [22.5322, 24.9996, 24.0414,  ..., 25.7002, 24.1045, 25.0527],\n",
       "         [24.7540, 24.6198, 25.9706,  ..., 25.5673, 25.5841, 25.7302],\n",
       "         [17.8433, 17.3420, 18.4870,  ..., 17.4362, 19.4744, 19.8112]]])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sounds_predict[20]  # visualisation du spectogram augmenté du 19 ième audio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "ce01a99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 64, 860]), torch.Size([2, 64, 860]))"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sounds_predict[20].shape, my_sounds_predict[1000].shape  # Tous nos audios sont uniformes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "083f1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.utils.data.DataLoader(my_sounds_predict) # On charge les données audios dans le DataLoader PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6b4e55",
   "metadata": {},
   "source": [
    "> Après ces étapes de processing, nous allons créer une fonction pour prédire les différents audios au travers de notre modèle de DL. Voici comment nous allons procéder :\n",
    "\n",
    "- Nous allons prédire les probabilités que dans un audio, il y ai un oiseau ou pas. Ces proba seront calculées avec la fonction **torch.softmax()**.  \n",
    "- Puis, avec ces probabilités prédites, nous pouvons déduire le label d'un audio avec la méthode **.argmax** sur l'objet qui contient les probabilités prédites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "f5c75bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model: torch.nn.Module, data:torch.utils.data.DataLoader, device: torch.device = device):\n",
    "    pred_probs = []\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for sample in data:\n",
    "            # Preparation du sample\n",
    "            sample = sample.to(device) # Envoyons sample sur GPU\n",
    "\n",
    "            # Forward pass (le model retourne un résultat du modèle logit brut)\n",
    "            pred_logit = model(sample)\n",
    "\n",
    "            # Obtention des probabilités prédites (logit -> probabilité prédite)\n",
    "            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0)\n",
    "\n",
    "            # Obtenez pred_prob du GPU pour d'autres calculs\n",
    "            pred_probs.append(pred_prob.cpu())\n",
    "            \n",
    "    # Empiler les pred_probs pour transformer la liste en tenseur\n",
    "    return torch.stack(pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "cd1455b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3846e-01, 8.6148e-01, 8.6221e-06,  ..., 9.1803e-06, 9.9807e-06,\n",
       "         9.1424e-06],\n",
       "        [9.4104e-01, 5.8840e-02, 1.6257e-05,  ..., 1.4621e-05, 1.3409e-05,\n",
       "         1.8374e-05],\n",
       "        [7.9159e-01, 2.0733e-01, 1.2109e-04,  ..., 9.6106e-05, 1.2486e-04,\n",
       "         1.5941e-04],\n",
       "        ...,\n",
       "        [6.0080e-01, 3.9891e-01, 3.8994e-05,  ..., 3.9644e-05, 3.7896e-05,\n",
       "         2.7837e-05],\n",
       "        [6.4643e-01, 3.5302e-01, 6.1608e-05,  ..., 6.3331e-05, 2.9521e-05,\n",
       "         8.4531e-05],\n",
       "        [8.3328e-01, 1.6651e-01, 2.2734e-05,  ..., 2.5535e-05, 2.5625e-05,\n",
       "         2.6073e-05]])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_probs = make_predictions(myModel,preds,device)\n",
    "preds_probs  # Probabilités prédites par le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1a16ee",
   "metadata": {},
   "source": [
    "> Nous affichons les 20 premières prédictions du fichier test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "1f787868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformez les probabilités de prédiction en label de prédiction avec argmax()\n",
    "pred_classes = preds_probs.argmax(dim=1)\n",
    "pred_classes[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30788a78",
   "metadata": {},
   "source": [
    "## CODE DE SAUVEGARDE DE NOS MODÈLES DE DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "9bd2d4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: /Users/nacersere/Downloads/models/04_pytorch_workflow_model_1_cross_validation.pth\n"
     ]
    }
   ],
   "source": [
    "# 1. Creation des liens de répertoire de nos modèles\n",
    "MODEL_PATH = Path(\"/Users/nacersere/Downloads/models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Creation d'un répertoire de sauvegarde du modèle\n",
    "MODEL_NAME = \"04_pytorch_workflow_model_1_cross_.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# 3. Sauvegarde du modèle\n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=myModel.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "           f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9efe40",
   "metadata": {},
   "source": [
    "> Nous avons sauvegardé trois modèles :\n",
    "\n",
    "- Le premier modèle est celui du gradient stochastique avec un learning_rate de 0.1 qui performe a ***87.95%***\n",
    "\n",
    "- Le second modèle est celui du modèle Adam avec un learning rate de 0.01 qui performe a ***86%***\n",
    "\n",
    "- Le troisème modèle est celui de la cross_validation que nous avons utilisé pour les prédictions et qui performe a ***89%***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fdfcac",
   "metadata": {},
   "source": [
    "> Liens vers les modèles : \n",
    "\n",
    "- SGD, learning_rate = 0.1 & performance ***87.95%*** = https://drive.google.com/file/d/1pAuGEJV15JU2dloxjyMP1v5ckBY00Sx9/view?usp=share_link\n",
    "\n",
    "- Adam, learning_rate = 0.01 & performance ***86%*** = https://drive.google.com/file/d/1RZ81p6etXCHr5qDTknGUu2wkAdFW8qvg/view?usp=share_link\n",
    "\n",
    "- Cross_Validation, performance ***89%*** = https://drive.google.com/file/d/1SrySL079U86sXL7yvspoH-wtI4NDgTw3/view?usp=share_link"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
